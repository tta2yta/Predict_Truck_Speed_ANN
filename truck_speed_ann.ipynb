{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of truck-ann.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tta2yta/Predict_Truck_Speed_ANN/blob/main/truck_speed_ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBjm8snzjKCJ"
      },
      "source": [
        "**Importing neccessary library packages** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWf-gWzYCtBR"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRTDxi4RCtBT"
      },
      "source": [
        "<b>Loading file to DataFrame</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "3lpG4nJJH2Of",
        "outputId": "a674f495-a2a1-4005-95c5-42a8241bec00"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b0f515c-8213-48f5-aea5-f4b06c1d2947\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b0f515c-8213-48f5-aea5-f4b06c1d2947\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving truck1.xlsx to truck1.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG8SiGOtLU50"
      },
      "source": [
        "import io\n",
        "df = pd.read_excel(io.BytesIO(uploaded['truck1.xlsx']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3tK6GoemHb9"
      },
      "source": [
        "**Displaying top 5 rows of the Truck dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Hzd1eZOKl-U1",
        "outputId": "0fb495ba-a463-4ebc-c851-6dcbe8cec79e"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>y</th>\n",
              "      <th>distance</th>\n",
              "      <th>Vehicle_type</th>\n",
              "      <th>Course</th>\n",
              "      <th>control_id</th>\n",
              "      <th>lon</th>\n",
              "      <th>speed</th>\n",
              "      <th>lat</th>\n",
              "      <th>fuel</th>\n",
              "      <th>link_quality</th>\n",
              "      <th>num_satellites</th>\n",
              "      <th>type</th>\n",
              "      <th>sender</th>\n",
              "      <th>received</th>\n",
              "      <th>vehicle_id</th>\n",
              "      <th>x</th>\n",
              "      <th>mes_number</th>\n",
              "      <th>fix_status</th>\n",
              "      <th>weight</th>\n",
              "      <th>gps_quality</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-02-28 08:02:19</td>\n",
              "      <td>5681383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>truck</td>\n",
              "      <td>251.0</td>\n",
              "      <td>31107</td>\n",
              "      <td>37.723074</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.255468</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>89</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>MessageDetector</td>\n",
              "      <td>43889.334942</td>\n",
              "      <td>9</td>\n",
              "      <td>7410971</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "      <td>63.836127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-02-28 08:02:19</td>\n",
              "      <td>5681383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>truck</td>\n",
              "      <td>251.0</td>\n",
              "      <td>31107</td>\n",
              "      <td>37.723074</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.255468</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>94</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>MessageDetector</td>\n",
              "      <td>43889.334942</td>\n",
              "      <td>9</td>\n",
              "      <td>7410971</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>63.836127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-02-28 08:02:19</td>\n",
              "      <td>5681844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>truck</td>\n",
              "      <td>125.0</td>\n",
              "      <td>31106</td>\n",
              "      <td>37.717123</td>\n",
              "      <td>30.0</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>1502.0</td>\n",
              "      <td>95</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>MessageDetector</td>\n",
              "      <td>43889.334942</td>\n",
              "      <td>8</td>\n",
              "      <td>7410564</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>-33.943062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-02-28 08:02:19</td>\n",
              "      <td>5681844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>truck</td>\n",
              "      <td>125.0</td>\n",
              "      <td>31106</td>\n",
              "      <td>37.717123</td>\n",
              "      <td>30.0</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>1502.0</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>MessageDetector</td>\n",
              "      <td>43889.334942</td>\n",
              "      <td>8</td>\n",
              "      <td>7410564</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>-33.943062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-02-28 08:02:19</td>\n",
              "      <td>5681844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>truck</td>\n",
              "      <td>125.0</td>\n",
              "      <td>31106</td>\n",
              "      <td>37.717123</td>\n",
              "      <td>30.0</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>1502.0</td>\n",
              "      <td>96</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>MessageDetector</td>\n",
              "      <td>43889.334942</td>\n",
              "      <td>8</td>\n",
              "      <td>7410564</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>99</td>\n",
              "      <td>-33.943062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Time        y  distance  ... weight  gps_quality     height\n",
              "0 2020-02-28 08:02:19  5681383       0.0  ...      0           91  63.836127\n",
              "1 2020-02-28 08:02:19  5681383       0.0  ...      0           94  63.836127\n",
              "2 2020-02-28 08:02:19  5681844       0.0  ...      0           94 -33.943062\n",
              "3 2020-02-28 08:02:19  5681844       0.0  ...      0          100 -33.943062\n",
              "4 2020-02-28 08:02:19  5681844       0.0  ...      0           99 -33.943062\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qj1bql2m-WG"
      },
      "source": [
        "**Viewing some statistical data like mean, count and standard deviation  for the dependent and independent variables.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "BU4EmuDDCtBT",
        "outputId": "b98fb196-a014-44cc-d378-cae15f472796"
      },
      "source": [
        "df[['lon','lat','height','Course','weight','speed']].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2342.000000</td>\n",
              "      <td>2342.000000</td>\n",
              "      <td>2342.000000</td>\n",
              "      <td>2342.000000</td>\n",
              "      <td>2342.000000</td>\n",
              "      <td>2342.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>37.719900</td>\n",
              "      <td>51.261040</td>\n",
              "      <td>-45.920367</td>\n",
              "      <td>-6.382021</td>\n",
              "      <td>24.612724</td>\n",
              "      <td>19.245342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.002332</td>\n",
              "      <td>0.002680</td>\n",
              "      <td>60.035549</td>\n",
              "      <td>110.533748</td>\n",
              "      <td>18.359220</td>\n",
              "      <td>9.658793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>37.716269</td>\n",
              "      <td>51.255481</td>\n",
              "      <td>-140.223617</td>\n",
              "      <td>-169.689844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>37.717921</td>\n",
              "      <td>51.259306</td>\n",
              "      <td>-107.119016</td>\n",
              "      <td>-113.164275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>37.720104</td>\n",
              "      <td>51.261075</td>\n",
              "      <td>-52.580483</td>\n",
              "      <td>22.199053</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.721755</td>\n",
              "      <td>51.263592</td>\n",
              "      <td>-2.503833</td>\n",
              "      <td>73.935004</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>28.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>37.724641</td>\n",
              "      <td>51.265437</td>\n",
              "      <td>68.874098</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>32.070000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               lon          lat  ...       weight        speed\n",
              "count  2342.000000  2342.000000  ...  2342.000000  2342.000000\n",
              "mean     37.719900    51.261040  ...    24.612724    19.245342\n",
              "std       0.002332     0.002680  ...    18.359220     9.658793\n",
              "min      37.716269    51.255481  ...     0.000000     0.000000\n",
              "25%      37.717921    51.259306  ...     0.000000    18.120000\n",
              "50%      37.720104    51.261075  ...    36.000000    20.000000\n",
              "75%      37.721755    51.263592  ...    39.000000    28.670000\n",
              "max      37.724641    51.265437  ...    45.000000    32.070000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvYjFoIwCtBU"
      },
      "source": [
        "<b>Display Unique values of column Vehicle Type</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YabtF6hGCtBU",
        "outputId": "c2a95416-870e-407e-cc52-d5ce3be12550"
      },
      "source": [
        "df['Vehicle_type'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['truck', 'shovel'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVcwLe6JCtBU"
      },
      "source": [
        "<b>Filtering the Truck data set with the following criteria\n",
        "1. selecting only truck values from Vehicle_type column\n",
        "\n",
        "2. the course column between -180 to 180\n",
        "\n",
        "3. and speed not equal to zero.\n",
        "\n",
        "---\n",
        "\n",
        "Type</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zxfDThmCtBU"
      },
      "source": [
        "df_Truck=df[(df.Vehicle_type=='truck') & (df.Course <= 180) & (df.Course >= -180) & (df.speed!=0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "3Z8dHn8ob9fH",
        "outputId": "ea175c9f-df73-4ad7-ea4f-d1e11b62eac7"
      },
      "source": [
        "df_truck_columns=df_truck_columns[(df_truck_columns.Course <= 180) & (df_truck_columns.Course <= 180) & (df_truck_columns.Course >= -180) & (df_truck_columns.speed!=0)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3fff6d2eb9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCourse\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCourse\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCourse\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_truck_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_truck_columns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbKavVmqwiL"
      },
      "source": [
        "**Slecting the independent and target variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy5k5IB7PtpE"
      },
      "source": [
        "\n",
        "df_truck_columns=df_Truck[['lon','lat','height','Course','weight','speed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "08TBefJkylle",
        "outputId": "d1391cd3-ba2b-4249-cfcb-9f105d6e3003"
      },
      "source": [
        "df_truck_columns.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>37.720177</td>\n",
              "      <td>51.260723</td>\n",
              "      <td>-40.285299</td>\n",
              "      <td>-11.244504</td>\n",
              "      <td>23.554832</td>\n",
              "      <td>22.511482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.002511</td>\n",
              "      <td>58.706278</td>\n",
              "      <td>100.820519</td>\n",
              "      <td>18.862163</td>\n",
              "      <td>5.938463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>37.716269</td>\n",
              "      <td>51.255481</td>\n",
              "      <td>-140.223617</td>\n",
              "      <td>-169.689844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>37.718601</td>\n",
              "      <td>51.259236</td>\n",
              "      <td>-87.515959</td>\n",
              "      <td>-112.799140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>37.720269</td>\n",
              "      <td>51.260679</td>\n",
              "      <td>-38.170880</td>\n",
              "      <td>-1.001522</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.721822</td>\n",
              "      <td>51.262248</td>\n",
              "      <td>3.724907</td>\n",
              "      <td>71.184066</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>29.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>37.724641</td>\n",
              "      <td>51.265437</td>\n",
              "      <td>68.874098</td>\n",
              "      <td>179.004937</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>32.070000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               lon          lat  ...       weight        speed\n",
              "count  1997.000000  1997.000000  ...  1997.000000  1997.000000\n",
              "mean     37.720177    51.260723  ...    23.554832    22.511482\n",
              "std       0.002113     0.002511  ...    18.862163     5.938463\n",
              "min      37.716269    51.255481  ...     0.000000     2.330000\n",
              "25%      37.718601    51.259236  ...     0.000000    19.020000\n",
              "50%      37.720269    51.260679  ...    36.000000    20.000000\n",
              "75%      37.721822    51.262248  ...    39.000000    29.020000\n",
              "max      37.724641    51.265437  ...    45.000000    32.070000\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES-mS4aZexGt",
        "outputId": "40c48ad5-8a1e-488a-9c98-498fe810a1d2"
      },
      "source": [
        "df_truck_columns.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lon       0\n",
              "lat       0\n",
              "height    0\n",
              "Course    0\n",
              "weight    0\n",
              "speed     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USgiOWlHq8zI"
      },
      "source": [
        "**Checking Null values in all the coulmns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsUxeRriQk4i",
        "outputId": "0fbed54c-395a-4bc6-8b26-82c09414c23d"
      },
      "source": [
        "\n",
        "print(df_truck_columns['lon'].isnull().sum())\n",
        "print(df_truck_columns['lat'].isnull().sum())\n",
        "print(df_truck_columns['height'].isnull().sum())\n",
        "print(df_truck_columns['Course'].isnull().sum())\n",
        "print(df_truck_columns['weight'].isnull().sum())\n",
        "print(df_truck_columns['speed'].isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfp8DBxmrNhj"
      },
      "source": [
        "**Checking data type of every column**<BR>\n",
        "AS it is displayed below all our data is numeric type. We do not have any object type that needs to be converted to numeric values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9kekqPQNYN5",
        "outputId": "81aea971-ad42-489d-a430-40162032f0f5"
      },
      "source": [
        "#check data type of every column \n",
        "df_truck_columns.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lon       float64\n",
              "lat       float64\n",
              "height    float64\n",
              "Course    float64\n",
              "weight      int64\n",
              "speed     float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmpU2kiLsCcT"
      },
      "source": [
        "<b>Checking for any empty value</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7isSfNpiOBut",
        "outputId": "c0e5b390-b4f7-449b-fc66-c5070d9b6e79"
      },
      "source": [
        "#check for nay empty value\n",
        "df_truck_columns[df_truck_columns['lon'] == ''].index \n",
        "df_Truck[df_truck_columns['lat'] == ''].index\n",
        "df_Truck[df_truck_columns['Course'] == ''].index\n",
        "df_Truck[df_truck_columns['weight'] == ''].index\n",
        "df_Truck[df_truck_columns['speed'] == ''].index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([], dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uilaoi4s8wQ"
      },
      "source": [
        "**Checking outliers**<br>\n",
        "An outlier is an observation point that is distant from other observations. we will discover outlies with visualization tool and remove them with IQR score<br>\n",
        "\n",
        "1.Discover outliers with visualization tools\n",
        "Box plot- a box plot is a method for graphically depicting groups of numerical data through their quartiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "nWGvvXGatBdX",
        "outputId": "224eca14-0b48-41bd-f5df-c5b2bd9e0f4b"
      },
      "source": [
        "sns.boxplot(x=df_truck_columns['lon'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3096212e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "03Xmaae5wA7d",
        "outputId": "148fb22e-3da3-4d48-bcd3-2b9c91c2d42d"
      },
      "source": [
        "sns.boxplot(x=df_truck_columns['lat'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3102d1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "iu5NQO-my6Ae",
        "outputId": "bd293b14-d28f-445e-8131-ac8d5c940313"
      },
      "source": [
        "sns.boxplot(x=df_truck_columns['height'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe315a0deb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "l649xVLXyFYA",
        "outputId": "af4f46c4-9d33-45a8-c47a-6d12047ec89a"
      },
      "source": [
        "sns.boxplot(x=df_truck_columns['Course'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe311a2a2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "uymKEhb8y0aL",
        "outputId": "43af9124-8d28-45f8-c914-3878a1834247"
      },
      "source": [
        "sns.boxplot(x=df_truck_columns['weight'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe313bb5630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqVHR_XUzGyj"
      },
      "source": [
        "**IQR-** The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 − Q1. <br>\n",
        "It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers. **bold text** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUSiCLZxY9j",
        "outputId": "32e097c5-75c7-433f-a468-4436675aa8f1"
      },
      "source": [
        "Q1 = df_truck_columns.quantile(0.25)\n",
        "Q3 = df_truck_columns.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(IQR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lon         0.008699\n",
            "lat         0.003555\n",
            "height     96.010640\n",
            "Course    184.452505\n",
            "weight    103.000000\n",
            "speed       9.960000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bgq_3imuy6J",
        "outputId": "fdf75ac9-3ed4-4b82-8e08-ef04c7c6ae9c"
      },
      "source": [
        "df_truck_columns.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(129630, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwrSokrfxjrP"
      },
      "source": [
        "df_truck_columns = df_truck_columns[~((df_truck_columns < (Q1 - 1.5 * IQR)) |(df_truck_columns > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LroFFsvmxxij",
        "outputId": "e802c8b3-68f0-4a56-d1b5-927fce5ed6c4"
      },
      "source": [
        "df_truck_columns.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(113245, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXCBxGt_2ely"
      },
      "source": [
        "**Replace values 0-empty when weight < 5; 1-loaded when weight > 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2p9uOGx_Z72",
        "outputId": "91c9b30c-8851-4b77-cfb4-edf157a2b5b3"
      },
      "source": [
        "df_truck_columns.loc[(df_Truck['weight'] <= 5),'weight']=0; df_truck_columns.loc[(df_Truck['weight'] > 5),'weight']=1;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "pg0ZRKgr2k6L",
        "outputId": "30aec780-242d-485c-f20f-cff2ab1fc69a"
      },
      "source": [
        "df_truck_columns.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37.717123</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>-33.943062</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37.717123</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>-33.943062</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.717123</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>-33.943062</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>37.717123</td>\n",
              "      <td>51.259548</td>\n",
              "      <td>-33.943062</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>37.716383</td>\n",
              "      <td>51.256480</td>\n",
              "      <td>68.614850</td>\n",
              "      <td>107.0</td>\n",
              "      <td>1</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          lon        lat     height  Course  weight  speed\n",
              "2   37.717123  51.259548 -33.943062   125.0       0   30.0\n",
              "3   37.717123  51.259548 -33.943062   125.0       0   30.0\n",
              "4   37.717123  51.259548 -33.943062   125.0       0   30.0\n",
              "5   37.717123  51.259548 -33.943062   125.0       0   30.0\n",
              "57  37.716383  51.256480  68.614850   107.0       1   20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJzz1SiCn4Hc",
        "outputId": "c56a9592-1a36-4392-8452-99beb07e55fd"
      },
      "source": [
        "decimals = 2    \r\n",
        "df_truck_columns['lon'] = df_truck_columns['lon'].apply(lambda x: round(x, decimals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj14P2I7oGb9",
        "outputId": "442f228f-e384-45d0-f4a6-52cd8e4c6891"
      },
      "source": [
        "df_truck_columns['lat'] = df_truck_columns['lat'].apply(lambda x: round(x, decimals))\r\n",
        "df_truck_columns['height'] = df_truck_columns['height'].apply(lambda x: round(x, decimals))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "10h1WxscoAxu",
        "outputId": "7f55caec-82a6-4cc1-ffa1-f929929c9cb8"
      },
      "source": [
        "df_truck_columns.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37.72</td>\n",
              "      <td>51.26</td>\n",
              "      <td>-33.94</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37.72</td>\n",
              "      <td>51.26</td>\n",
              "      <td>-33.94</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.72</td>\n",
              "      <td>51.26</td>\n",
              "      <td>-33.94</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>37.72</td>\n",
              "      <td>51.26</td>\n",
              "      <td>-33.94</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>37.72</td>\n",
              "      <td>51.26</td>\n",
              "      <td>68.61</td>\n",
              "      <td>107.0</td>\n",
              "      <td>1</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      lon    lat  height  Course  weight  speed\n",
              "2   37.72  51.26  -33.94   125.0       0   30.0\n",
              "3   37.72  51.26  -33.94   125.0       0   30.0\n",
              "4   37.72  51.26  -33.94   125.0       0   30.0\n",
              "5   37.72  51.26  -33.94   125.0       0   30.0\n",
              "57  37.72  51.26   68.61   107.0       1   20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-GuYjkm0j7J"
      },
      "source": [
        "**Lets now determine the correlation between dependent variable(speed) and independent variable(lon, lat, height, weight, Course)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "B8XrSHWMRXWu",
        "outputId": "6249fb5c-330b-48ae-b89e-77cbb72e382b"
      },
      "source": [
        "# check correlation between independent variablea and dependent variable\n",
        "df_truck_columns.corr()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lon</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.038513</td>\n",
              "      <td>0.139076</td>\n",
              "      <td>-0.146169</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>-0.017759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lat</th>\n",
              "      <td>0.038513</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.645342</td>\n",
              "      <td>-0.076539</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.005388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>height</th>\n",
              "      <td>0.139076</td>\n",
              "      <td>-0.645342</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.020099</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>-0.012337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Course</th>\n",
              "      <td>-0.146169</td>\n",
              "      <td>-0.076539</td>\n",
              "      <td>0.020099</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.055692</td>\n",
              "      <td>0.054487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight</th>\n",
              "      <td>-0.001355</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.003617</td>\n",
              "      <td>-0.055692</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.897344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speed</th>\n",
              "      <td>-0.017759</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>-0.012337</td>\n",
              "      <td>0.054487</td>\n",
              "      <td>-0.897344</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             lon       lat    height    Course    weight     speed\n",
              "lon     1.000000  0.038513  0.139076 -0.146169 -0.001355 -0.017759\n",
              "lat     0.038513  1.000000 -0.645342 -0.076539  0.002324  0.005388\n",
              "height  0.139076 -0.645342  1.000000  0.020099  0.003617 -0.012337\n",
              "Course -0.146169 -0.076539  0.020099  1.000000 -0.055692  0.054487\n",
              "weight -0.001355  0.002324  0.003617 -0.055692  1.000000 -0.897344\n",
              "speed  -0.017759  0.005388 -0.012337  0.054487 -0.897344  1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GZSvcBn1b5q"
      },
      "source": [
        "**Placing the input variables(['lon','lat','height','Course','weight']) into separate dataframe variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0k8eyrhgWVy"
      },
      "source": [
        "df_truck_input=df_truck_columns[['lon','lat','height','Course','weight']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQQRzhuk1xaR"
      },
      "source": [
        "**Placing the output variables(['speed']) into separate dataframe variable.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2q-gHIxsju3"
      },
      "source": [
        "df_truck_output=df_truck_columns[['speed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YReAwikC37RP"
      },
      "source": [
        "**MinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EDoJSs1tE2i"
      },
      "source": [
        "#Using MinMaxScaler we scale the continuous input variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_truck_input_scaled = pd.DataFrame(scaler.fit_transform(df_truck_input), columns=df_truck_input.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dp3nT5rCtBZ"
      },
      "source": [
        "<b>Initial sample split of input variables: 70% for training, 15% for validation, 15% for testing</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzNClvKCtBZ"
      },
      "source": [
        " x_train, x_validate, x_test = np.split(df_truck_input_scaled.sample(frac=1, random_state=42), [int(.7*len(df_truck_input)), int(.85*len(df_truck_input))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "q7EY8uOhCtBZ",
        "outputId": "af75ab06-602d-4522-e0d2-a6eee73f2e4e"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>height</th>\n",
              "      <th>Course</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>125764</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.890373</td>\n",
              "      <td>0.265067</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24515</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.937032</td>\n",
              "      <td>0.703068</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34023</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.975001</td>\n",
              "      <td>0.012850</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23986</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.735799</td>\n",
              "      <td>0.900358</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119737</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.839005</td>\n",
              "      <td>0.148928</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42112</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.258465</td>\n",
              "      <td>0.802826</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64147</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.895595</td>\n",
              "      <td>0.582307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128883</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.156072</td>\n",
              "      <td>0.259303</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126505</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.443346</td>\n",
              "      <td>0.117195</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.620607</td>\n",
              "      <td>0.192696</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90741 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         lon  lat    height    Course  weight\n",
              "125764  0.25  0.5  0.890373  0.265067     0.0\n",
              "24515   0.50  0.5  0.937032  0.703068     1.0\n",
              "34023   0.25  0.0  0.975001  0.012850     1.0\n",
              "23986   0.00  0.5  0.735799  0.900358     1.0\n",
              "119737  0.75  0.5  0.839005  0.148928     1.0\n",
              "...      ...  ...       ...       ...     ...\n",
              "42112   0.25  0.5  0.258465  0.802826     0.0\n",
              "64147   0.25  0.0  0.895595  0.582307     0.0\n",
              "128883  0.25  0.5  0.156072  0.259303     0.0\n",
              "126505  0.25  0.5  0.443346  0.117195     1.0\n",
              "105     0.25  0.5  0.620607  0.192696     0.0\n",
              "\n",
              "[90741 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRf2rl2I4eIy"
      },
      "source": [
        "\n",
        "**Initial sample split of output variable: 70% for training, 15% for validation, 15% for testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJZI-iukCtBZ"
      },
      "source": [
        "y_train, y_validate, y_test = np.split(df_truck_output.sample(frac=1, random_state=42), [int(.7*len(df_truck_output)), int(.85*len(df_truck_output))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIdzYvfjCtBZ",
        "outputId": "dd7c6cdb-de73-40e8-ca74-560829e57865"
      },
      "source": [
        "y_train.shape, x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((90741, 1), (90741, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovs1q03uAnpI"
      },
      "source": [
        "valid_set=(x_validate, y_validate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khhiDxcmBVhe",
        "outputId": "f1161f5f-af9a-49bb-c01b-4d8724948077"
      },
      "source": [
        "valid_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(             lon       lat    height    Course    weight\n",
              "  100638  0.371669  0.408345  0.651535  0.747749  0.470588\n",
              "  74864   0.333035  0.510307  0.264056  0.151896  0.487395\n",
              "  41123   0.250767  0.390234  0.450364  0.639399  0.000000\n",
              "  91615   0.129107  0.667451  0.452845  0.159936  0.155462\n",
              "  87411   0.330869  0.508800  0.263771  0.151896  0.155462\n",
              "  ...          ...       ...       ...       ...       ...\n",
              "  17215   0.243561  0.862004  0.837852  0.243302  0.378151\n",
              "  47475   0.339787  0.879219  0.812909  0.721229  0.000000\n",
              "  129400  0.369451  0.408203  0.648568  0.712089  0.176471\n",
              "  111409  0.921448  0.374390  0.781446  0.302652  0.525210\n",
              "  18227   0.411767  0.128665  0.891913  0.569981  0.168067\n",
              "  \n",
              "  [19444 rows x 5 columns],         speed\n",
              "  376097  20.00\n",
              "  279697  19.10\n",
              "  153427  31.59\n",
              "  342156  20.00\n",
              "  326473  20.00\n",
              "  ...       ...\n",
              "  64403   18.88\n",
              "  176998  28.47\n",
              "  483301  20.24\n",
              "  416139  20.00\n",
              "  68109   21.02\n",
              "  \n",
              "  [19444 rows x 1 columns])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TliGQ8BMnLXE"
      },
      "source": [
        "**Define Keras Model**<br>\n",
        "I have used a Sequential model and add layers one at a time until we are happy with our network architecture.The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the **input_dim** argument and setting it to 5 for the 5 input variables.<br>\n",
        "Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument.Regarding to this dataset I have used three hidden layers having each 64, 32 and 16 neurons respectivelly.The first two hidden layes uses activation function **sigmoid** while the third hidden layer uses **Relu** activation function. I have used Relu activation function for the output layer because the output variable is continous numeric value and as he range of ReLu is [0, inf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O67v4rD0CtBZ"
      },
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=5, activation='sigmoid'))\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_YPps1fpm0i"
      },
      "source": [
        "I have used loss type **mean_square_error** with an optimizer called **adam**.**Adam** is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccs78-NbMGbl"
      },
      "source": [
        "# compile the keras model  #binary_crossentropy\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['MSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ete6FjktpwRg"
      },
      "source": [
        "Then we fit the defined model with our training and validation sample datasets.<br>\n",
        "**Training Dataset:** The sample of data used to fit the model.<br>\n",
        "**Validation Dataset:** The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.<br>\n",
        "**Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.<br>\n",
        "\n",
        "The **batch size** is a number of samples processed before the model is updated.The batch_size is set to 100 for this model.The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.<br>\n",
        "\n",
        "The number of **epochs** is the number of complete passes through the training dataset. The number of epoches is set to 1000 for this model.\n",
        "The number of epochs can be set to an integer value between one and infinity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g986AliMMA5",
        "outputId": "bc60ecc4-d6c4-4b60-f0ce-25da13c128b9"
      },
      "source": [
        "# fit the keras model on the dataset\n",
        "model.fit(x_train, y_train, epochs=1000, validation_data=valid_set, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 78.4113 - MSE: 78.4113 - val_loss: 31.4479 - val_MSE: 31.4479\n",
            "Epoch 2/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 14.7332 - MSE: 14.7332 - val_loss: 7.9463 - val_MSE: 7.9463\n",
            "Epoch 3/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.1149 - MSE: 8.1149 - val_loss: 7.9305 - val_MSE: 7.9305\n",
            "Epoch 4/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.1024 - MSE: 8.1024 - val_loss: 7.9529 - val_MSE: 7.9529\n",
            "Epoch 5/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.1020 - MSE: 8.1020 - val_loss: 8.0732 - val_MSE: 8.0732\n",
            "Epoch 6/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.1031 - MSE: 8.1031 - val_loss: 7.9042 - val_MSE: 7.9042\n",
            "Epoch 7/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.1019 - MSE: 8.1019 - val_loss: 7.9038 - val_MSE: 7.9038\n",
            "Epoch 8/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0911 - MSE: 8.0911 - val_loss: 7.9395 - val_MSE: 7.9395\n",
            "Epoch 9/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0883 - MSE: 8.0883 - val_loss: 7.9265 - val_MSE: 7.9265\n",
            "Epoch 10/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0834 - MSE: 8.0834 - val_loss: 7.9018 - val_MSE: 7.9018\n",
            "Epoch 11/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0754 - MSE: 8.0754 - val_loss: 7.8967 - val_MSE: 7.8967\n",
            "Epoch 12/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0813 - MSE: 8.0813 - val_loss: 7.8841 - val_MSE: 7.8841\n",
            "Epoch 13/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0659 - MSE: 8.0659 - val_loss: 7.9026 - val_MSE: 7.9026\n",
            "Epoch 14/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0581 - MSE: 8.0581 - val_loss: 7.8518 - val_MSE: 7.8518\n",
            "Epoch 15/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0484 - MSE: 8.0484 - val_loss: 7.8681 - val_MSE: 7.8681\n",
            "Epoch 16/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 8.0210 - MSE: 8.0210 - val_loss: 7.8145 - val_MSE: 7.8145\n",
            "Epoch 17/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.9820 - MSE: 7.9820 - val_loss: 7.7691 - val_MSE: 7.7691\n",
            "Epoch 18/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.9417 - MSE: 7.9417 - val_loss: 7.6968 - val_MSE: 7.6968\n",
            "Epoch 19/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.8760 - MSE: 7.8760 - val_loss: 7.6327 - val_MSE: 7.6327\n",
            "Epoch 20/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.8193 - MSE: 7.8193 - val_loss: 7.5948 - val_MSE: 7.5948\n",
            "Epoch 21/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7967 - MSE: 7.7967 - val_loss: 7.7338 - val_MSE: 7.7338\n",
            "Epoch 22/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7803 - MSE: 7.7803 - val_loss: 7.5555 - val_MSE: 7.5555\n",
            "Epoch 23/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7552 - MSE: 7.7552 - val_loss: 7.5372 - val_MSE: 7.5372\n",
            "Epoch 24/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7531 - MSE: 7.7531 - val_loss: 7.6198 - val_MSE: 7.6198\n",
            "Epoch 25/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7424 - MSE: 7.7424 - val_loss: 7.5097 - val_MSE: 7.5097\n",
            "Epoch 26/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7191 - MSE: 7.7191 - val_loss: 7.5131 - val_MSE: 7.5131\n",
            "Epoch 27/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7108 - MSE: 7.7108 - val_loss: 7.5159 - val_MSE: 7.5159\n",
            "Epoch 28/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.7045 - MSE: 7.7045 - val_loss: 7.5956 - val_MSE: 7.5956\n",
            "Epoch 29/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6977 - MSE: 7.6977 - val_loss: 7.5580 - val_MSE: 7.5580\n",
            "Epoch 30/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6898 - MSE: 7.6898 - val_loss: 7.4838 - val_MSE: 7.4838\n",
            "Epoch 31/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6777 - MSE: 7.6777 - val_loss: 7.5987 - val_MSE: 7.5987\n",
            "Epoch 32/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6740 - MSE: 7.6740 - val_loss: 7.4884 - val_MSE: 7.4884\n",
            "Epoch 33/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6669 - MSE: 7.6669 - val_loss: 7.5092 - val_MSE: 7.5092\n",
            "Epoch 34/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6571 - MSE: 7.6571 - val_loss: 7.4628 - val_MSE: 7.4628\n",
            "Epoch 35/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6467 - MSE: 7.6467 - val_loss: 7.4367 - val_MSE: 7.4367\n",
            "Epoch 36/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6393 - MSE: 7.6393 - val_loss: 7.4674 - val_MSE: 7.4674\n",
            "Epoch 37/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6287 - MSE: 7.6287 - val_loss: 7.4432 - val_MSE: 7.4432\n",
            "Epoch 38/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6262 - MSE: 7.6262 - val_loss: 7.4728 - val_MSE: 7.4728\n",
            "Epoch 39/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6168 - MSE: 7.6168 - val_loss: 7.4038 - val_MSE: 7.4038\n",
            "Epoch 40/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6055 - MSE: 7.6055 - val_loss: 7.3947 - val_MSE: 7.3947\n",
            "Epoch 41/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.6010 - MSE: 7.6010 - val_loss: 7.3794 - val_MSE: 7.3794\n",
            "Epoch 42/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5938 - MSE: 7.5938 - val_loss: 7.3903 - val_MSE: 7.3903\n",
            "Epoch 43/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5870 - MSE: 7.5870 - val_loss: 7.3765 - val_MSE: 7.3765\n",
            "Epoch 44/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5802 - MSE: 7.5802 - val_loss: 7.3817 - val_MSE: 7.3817\n",
            "Epoch 45/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5678 - MSE: 7.5678 - val_loss: 7.3490 - val_MSE: 7.3490\n",
            "Epoch 46/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5567 - MSE: 7.5567 - val_loss: 7.3414 - val_MSE: 7.3414\n",
            "Epoch 47/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5509 - MSE: 7.5509 - val_loss: 7.3452 - val_MSE: 7.3452\n",
            "Epoch 48/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5453 - MSE: 7.5453 - val_loss: 7.3284 - val_MSE: 7.3284\n",
            "Epoch 49/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5343 - MSE: 7.5343 - val_loss: 7.3323 - val_MSE: 7.3323\n",
            "Epoch 50/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5304 - MSE: 7.5304 - val_loss: 7.3037 - val_MSE: 7.3037\n",
            "Epoch 51/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5208 - MSE: 7.5208 - val_loss: 7.3241 - val_MSE: 7.3241\n",
            "Epoch 52/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.5019 - MSE: 7.5019 - val_loss: 7.3086 - val_MSE: 7.3086\n",
            "Epoch 53/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4965 - MSE: 7.4965 - val_loss: 7.2702 - val_MSE: 7.2702\n",
            "Epoch 54/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4794 - MSE: 7.4794 - val_loss: 7.3069 - val_MSE: 7.3069\n",
            "Epoch 55/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4745 - MSE: 7.4745 - val_loss: 7.2404 - val_MSE: 7.2404\n",
            "Epoch 56/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4466 - MSE: 7.4466 - val_loss: 7.2491 - val_MSE: 7.2491\n",
            "Epoch 57/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4462 - MSE: 7.4462 - val_loss: 7.2161 - val_MSE: 7.2161\n",
            "Epoch 58/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4231 - MSE: 7.4231 - val_loss: 7.2180 - val_MSE: 7.2180\n",
            "Epoch 59/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.4024 - MSE: 7.4024 - val_loss: 7.2040 - val_MSE: 7.2040\n",
            "Epoch 60/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.3725 - MSE: 7.3725 - val_loss: 7.1726 - val_MSE: 7.1726\n",
            "Epoch 61/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.3608 - MSE: 7.3608 - val_loss: 7.1302 - val_MSE: 7.1302\n",
            "Epoch 62/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.3333 - MSE: 7.3333 - val_loss: 7.0885 - val_MSE: 7.0885\n",
            "Epoch 63/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.3160 - MSE: 7.3160 - val_loss: 7.0739 - val_MSE: 7.0739\n",
            "Epoch 64/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.2952 - MSE: 7.2952 - val_loss: 7.0875 - val_MSE: 7.0875\n",
            "Epoch 65/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.2705 - MSE: 7.2705 - val_loss: 7.0400 - val_MSE: 7.0400\n",
            "Epoch 66/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.2576 - MSE: 7.2576 - val_loss: 7.0974 - val_MSE: 7.0974\n",
            "Epoch 67/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.2257 - MSE: 7.2257 - val_loss: 6.9869 - val_MSE: 6.9869\n",
            "Epoch 68/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.1995 - MSE: 7.1995 - val_loss: 6.9523 - val_MSE: 6.9523\n",
            "Epoch 69/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.1763 - MSE: 7.1763 - val_loss: 6.9377 - val_MSE: 6.9377\n",
            "Epoch 70/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.1583 - MSE: 7.1583 - val_loss: 6.8998 - val_MSE: 6.8998\n",
            "Epoch 71/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.1394 - MSE: 7.1394 - val_loss: 6.8744 - val_MSE: 6.8744\n",
            "Epoch 72/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.1104 - MSE: 7.1104 - val_loss: 6.8375 - val_MSE: 6.8375\n",
            "Epoch 73/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.0887 - MSE: 7.0887 - val_loss: 6.8256 - val_MSE: 6.8256\n",
            "Epoch 74/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.0599 - MSE: 7.0599 - val_loss: 6.7948 - val_MSE: 6.7948\n",
            "Epoch 75/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.0407 - MSE: 7.0407 - val_loss: 6.8421 - val_MSE: 6.8421\n",
            "Epoch 76/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 7.0092 - MSE: 7.0092 - val_loss: 6.7954 - val_MSE: 6.7954\n",
            "Epoch 77/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.9865 - MSE: 6.9865 - val_loss: 6.7228 - val_MSE: 6.7228\n",
            "Epoch 78/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.9743 - MSE: 6.9743 - val_loss: 6.6990 - val_MSE: 6.6990\n",
            "Epoch 79/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.9386 - MSE: 6.9386 - val_loss: 6.7634 - val_MSE: 6.7634\n",
            "Epoch 80/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.9117 - MSE: 6.9117 - val_loss: 6.6464 - val_MSE: 6.6464\n",
            "Epoch 81/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.8770 - MSE: 6.8770 - val_loss: 6.6148 - val_MSE: 6.6148\n",
            "Epoch 82/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.8529 - MSE: 6.8529 - val_loss: 6.5883 - val_MSE: 6.5883\n",
            "Epoch 83/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.8208 - MSE: 6.8208 - val_loss: 6.5965 - val_MSE: 6.5965\n",
            "Epoch 84/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.8010 - MSE: 6.8010 - val_loss: 6.5774 - val_MSE: 6.5774\n",
            "Epoch 85/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.7678 - MSE: 6.7678 - val_loss: 6.4880 - val_MSE: 6.4880\n",
            "Epoch 86/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.7232 - MSE: 6.7232 - val_loss: 6.4894 - val_MSE: 6.4894\n",
            "Epoch 87/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.7033 - MSE: 6.7033 - val_loss: 6.4438 - val_MSE: 6.4438\n",
            "Epoch 88/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.6701 - MSE: 6.6701 - val_loss: 6.4139 - val_MSE: 6.4139\n",
            "Epoch 89/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.6422 - MSE: 6.6422 - val_loss: 6.3546 - val_MSE: 6.3546\n",
            "Epoch 90/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.6121 - MSE: 6.6121 - val_loss: 6.3137 - val_MSE: 6.3137\n",
            "Epoch 91/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.5797 - MSE: 6.5797 - val_loss: 6.3007 - val_MSE: 6.3007\n",
            "Epoch 92/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.5562 - MSE: 6.5562 - val_loss: 6.4494 - val_MSE: 6.4494\n",
            "Epoch 93/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.5344 - MSE: 6.5344 - val_loss: 6.2585 - val_MSE: 6.2585\n",
            "Epoch 94/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.5060 - MSE: 6.5060 - val_loss: 6.2254 - val_MSE: 6.2254\n",
            "Epoch 95/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.4842 - MSE: 6.4842 - val_loss: 6.1940 - val_MSE: 6.1940\n",
            "Epoch 96/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.4619 - MSE: 6.4619 - val_loss: 6.2383 - val_MSE: 6.2383\n",
            "Epoch 97/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.4441 - MSE: 6.4441 - val_loss: 6.1774 - val_MSE: 6.1774\n",
            "Epoch 98/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.4301 - MSE: 6.4301 - val_loss: 6.1424 - val_MSE: 6.1424\n",
            "Epoch 99/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.4062 - MSE: 6.4062 - val_loss: 6.1225 - val_MSE: 6.1225\n",
            "Epoch 100/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3899 - MSE: 6.3899 - val_loss: 6.1218 - val_MSE: 6.1218\n",
            "Epoch 101/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3768 - MSE: 6.3768 - val_loss: 6.1229 - val_MSE: 6.1229\n",
            "Epoch 102/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3612 - MSE: 6.3612 - val_loss: 6.1201 - val_MSE: 6.1201\n",
            "Epoch 103/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3408 - MSE: 6.3408 - val_loss: 6.0757 - val_MSE: 6.0757\n",
            "Epoch 104/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3245 - MSE: 6.3245 - val_loss: 6.0343 - val_MSE: 6.0343\n",
            "Epoch 105/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3250 - MSE: 6.3250 - val_loss: 6.1811 - val_MSE: 6.1811\n",
            "Epoch 106/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.3121 - MSE: 6.3121 - val_loss: 6.0300 - val_MSE: 6.0300\n",
            "Epoch 107/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2948 - MSE: 6.2948 - val_loss: 6.0258 - val_MSE: 6.0258\n",
            "Epoch 108/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2890 - MSE: 6.2890 - val_loss: 6.0105 - val_MSE: 6.0105\n",
            "Epoch 109/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2762 - MSE: 6.2762 - val_loss: 6.0097 - val_MSE: 6.0097\n",
            "Epoch 110/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2610 - MSE: 6.2610 - val_loss: 6.0310 - val_MSE: 6.0310\n",
            "Epoch 111/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2555 - MSE: 6.2555 - val_loss: 6.1140 - val_MSE: 6.1140\n",
            "Epoch 112/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2337 - MSE: 6.2337 - val_loss: 6.0162 - val_MSE: 6.0162\n",
            "Epoch 113/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2312 - MSE: 6.2312 - val_loss: 5.9514 - val_MSE: 5.9514\n",
            "Epoch 114/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2209 - MSE: 6.2209 - val_loss: 5.9406 - val_MSE: 5.9406\n",
            "Epoch 115/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.2061 - MSE: 6.2061 - val_loss: 6.1012 - val_MSE: 6.1012\n",
            "Epoch 116/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 6.1997 - MSE: 6.1997 - val_loss: 5.9754 - val_MSE: 5.9754\n",
            "Epoch 117/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 6.1825 - MSE: 6.1825 - val_loss: 5.9309 - val_MSE: 5.9309\n",
            "Epoch 118/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 6.1706 - MSE: 6.1706 - val_loss: 5.9357 - val_MSE: 5.9357\n",
            "Epoch 119/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 6.1635 - MSE: 6.1635 - val_loss: 5.9262 - val_MSE: 5.9262\n",
            "Epoch 120/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.1458 - MSE: 6.1458 - val_loss: 5.9225 - val_MSE: 5.9225\n",
            "Epoch 121/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.1340 - MSE: 6.1340 - val_loss: 5.9147 - val_MSE: 5.9147\n",
            "Epoch 122/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.1279 - MSE: 6.1279 - val_loss: 5.9017 - val_MSE: 5.9017\n",
            "Epoch 123/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.1190 - MSE: 6.1190 - val_loss: 5.8400 - val_MSE: 5.8400\n",
            "Epoch 124/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.1046 - MSE: 6.1046 - val_loss: 5.8865 - val_MSE: 5.8865\n",
            "Epoch 125/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0898 - MSE: 6.0898 - val_loss: 5.8374 - val_MSE: 5.8374\n",
            "Epoch 126/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0749 - MSE: 6.0749 - val_loss: 5.8047 - val_MSE: 5.8047\n",
            "Epoch 127/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0588 - MSE: 6.0588 - val_loss: 5.8059 - val_MSE: 5.8059\n",
            "Epoch 128/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0463 - MSE: 6.0463 - val_loss: 5.8126 - val_MSE: 5.8126\n",
            "Epoch 129/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0407 - MSE: 6.0407 - val_loss: 5.8222 - val_MSE: 5.8222\n",
            "Epoch 130/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0349 - MSE: 6.0349 - val_loss: 5.8244 - val_MSE: 5.8244\n",
            "Epoch 131/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0128 - MSE: 6.0128 - val_loss: 5.7789 - val_MSE: 5.7789\n",
            "Epoch 132/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 6.0003 - MSE: 6.0003 - val_loss: 5.7581 - val_MSE: 5.7581\n",
            "Epoch 133/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9924 - MSE: 5.9924 - val_loss: 5.7565 - val_MSE: 5.7565\n",
            "Epoch 134/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9914 - MSE: 5.9914 - val_loss: 5.7816 - val_MSE: 5.7816\n",
            "Epoch 135/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9831 - MSE: 5.9831 - val_loss: 5.7486 - val_MSE: 5.7486\n",
            "Epoch 136/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9585 - MSE: 5.9585 - val_loss: 5.7249 - val_MSE: 5.7249\n",
            "Epoch 137/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9469 - MSE: 5.9469 - val_loss: 5.7292 - val_MSE: 5.7292\n",
            "Epoch 138/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9341 - MSE: 5.9341 - val_loss: 5.8251 - val_MSE: 5.8251\n",
            "Epoch 139/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9320 - MSE: 5.9320 - val_loss: 5.7061 - val_MSE: 5.7061\n",
            "Epoch 140/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9127 - MSE: 5.9127 - val_loss: 5.6851 - val_MSE: 5.6851\n",
            "Epoch 141/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9085 - MSE: 5.9085 - val_loss: 5.8086 - val_MSE: 5.8086\n",
            "Epoch 142/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.9031 - MSE: 5.9031 - val_loss: 5.6770 - val_MSE: 5.6770\n",
            "Epoch 143/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8832 - MSE: 5.8832 - val_loss: 5.6762 - val_MSE: 5.6762\n",
            "Epoch 144/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8690 - MSE: 5.8690 - val_loss: 5.6749 - val_MSE: 5.6749\n",
            "Epoch 145/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8613 - MSE: 5.8613 - val_loss: 5.7009 - val_MSE: 5.7009\n",
            "Epoch 146/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8593 - MSE: 5.8593 - val_loss: 5.6567 - val_MSE: 5.6567\n",
            "Epoch 147/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8464 - MSE: 5.8464 - val_loss: 5.6584 - val_MSE: 5.6584\n",
            "Epoch 148/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8406 - MSE: 5.8406 - val_loss: 5.6037 - val_MSE: 5.6037\n",
            "Epoch 149/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8264 - MSE: 5.8264 - val_loss: 5.6230 - val_MSE: 5.6230\n",
            "Epoch 150/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8341 - MSE: 5.8341 - val_loss: 5.5994 - val_MSE: 5.5994\n",
            "Epoch 151/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8130 - MSE: 5.8130 - val_loss: 5.5978 - val_MSE: 5.5978\n",
            "Epoch 152/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.8063 - MSE: 5.8063 - val_loss: 5.5910 - val_MSE: 5.5910\n",
            "Epoch 153/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7976 - MSE: 5.7976 - val_loss: 5.5548 - val_MSE: 5.5548\n",
            "Epoch 154/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7859 - MSE: 5.7859 - val_loss: 5.5824 - val_MSE: 5.5824\n",
            "Epoch 155/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7771 - MSE: 5.7771 - val_loss: 5.6002 - val_MSE: 5.6002\n",
            "Epoch 156/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7695 - MSE: 5.7695 - val_loss: 5.5977 - val_MSE: 5.5977\n",
            "Epoch 157/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7567 - MSE: 5.7567 - val_loss: 5.5874 - val_MSE: 5.5874\n",
            "Epoch 158/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7516 - MSE: 5.7516 - val_loss: 5.6062 - val_MSE: 5.6062\n",
            "Epoch 159/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7410 - MSE: 5.7410 - val_loss: 5.5612 - val_MSE: 5.5612\n",
            "Epoch 160/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7352 - MSE: 5.7352 - val_loss: 5.5230 - val_MSE: 5.5230\n",
            "Epoch 161/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7197 - MSE: 5.7197 - val_loss: 5.5033 - val_MSE: 5.5033\n",
            "Epoch 162/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7154 - MSE: 5.7154 - val_loss: 5.5021 - val_MSE: 5.5021\n",
            "Epoch 163/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.7118 - MSE: 5.7118 - val_loss: 5.4775 - val_MSE: 5.4775\n",
            "Epoch 164/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6924 - MSE: 5.6924 - val_loss: 5.5019 - val_MSE: 5.5019\n",
            "Epoch 165/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6919 - MSE: 5.6919 - val_loss: 5.4866 - val_MSE: 5.4866\n",
            "Epoch 166/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6781 - MSE: 5.6781 - val_loss: 5.4862 - val_MSE: 5.4862\n",
            "Epoch 167/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6639 - MSE: 5.6639 - val_loss: 5.4513 - val_MSE: 5.4513\n",
            "Epoch 168/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6553 - MSE: 5.6553 - val_loss: 5.5329 - val_MSE: 5.5329\n",
            "Epoch 169/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6496 - MSE: 5.6496 - val_loss: 5.4524 - val_MSE: 5.4524\n",
            "Epoch 170/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6503 - MSE: 5.6503 - val_loss: 5.4145 - val_MSE: 5.4145\n",
            "Epoch 171/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6239 - MSE: 5.6239 - val_loss: 5.4191 - val_MSE: 5.4191\n",
            "Epoch 172/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6151 - MSE: 5.6151 - val_loss: 5.4364 - val_MSE: 5.4364\n",
            "Epoch 173/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.6056 - MSE: 5.6056 - val_loss: 5.3855 - val_MSE: 5.3855\n",
            "Epoch 174/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5989 - MSE: 5.5989 - val_loss: 5.4091 - val_MSE: 5.4091\n",
            "Epoch 175/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5947 - MSE: 5.5947 - val_loss: 5.4109 - val_MSE: 5.4109\n",
            "Epoch 176/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5825 - MSE: 5.5825 - val_loss: 5.3611 - val_MSE: 5.3611\n",
            "Epoch 177/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5772 - MSE: 5.5772 - val_loss: 5.4007 - val_MSE: 5.4007\n",
            "Epoch 178/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5608 - MSE: 5.5608 - val_loss: 5.3565 - val_MSE: 5.3565\n",
            "Epoch 179/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5417 - MSE: 5.5417 - val_loss: 5.3740 - val_MSE: 5.3740\n",
            "Epoch 180/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5397 - MSE: 5.5397 - val_loss: 5.3720 - val_MSE: 5.3720\n",
            "Epoch 181/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.5364 - MSE: 5.5364 - val_loss: 5.3678 - val_MSE: 5.3678\n",
            "Epoch 182/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.5209 - MSE: 5.5209 - val_loss: 5.3702 - val_MSE: 5.3702\n",
            "Epoch 183/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.5094 - MSE: 5.5094 - val_loss: 5.3312 - val_MSE: 5.3312\n",
            "Epoch 184/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.5082 - MSE: 5.5082 - val_loss: 5.4066 - val_MSE: 5.4066\n",
            "Epoch 185/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.5058 - MSE: 5.5058 - val_loss: 5.3172 - val_MSE: 5.3172\n",
            "Epoch 186/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.4842 - MSE: 5.4842 - val_loss: 5.3453 - val_MSE: 5.3453\n",
            "Epoch 187/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.4835 - MSE: 5.4835 - val_loss: 5.2734 - val_MSE: 5.2734\n",
            "Epoch 188/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.4738 - MSE: 5.4738 - val_loss: 5.3071 - val_MSE: 5.3071\n",
            "Epoch 189/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4729 - MSE: 5.4729 - val_loss: 5.2742 - val_MSE: 5.2742\n",
            "Epoch 190/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4590 - MSE: 5.4590 - val_loss: 5.3106 - val_MSE: 5.3106\n",
            "Epoch 191/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4350 - MSE: 5.4350 - val_loss: 5.2747 - val_MSE: 5.2747\n",
            "Epoch 192/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4421 - MSE: 5.4421 - val_loss: 5.2633 - val_MSE: 5.2633\n",
            "Epoch 193/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4325 - MSE: 5.4325 - val_loss: 5.2289 - val_MSE: 5.2289\n",
            "Epoch 194/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4197 - MSE: 5.4197 - val_loss: 5.3021 - val_MSE: 5.3021\n",
            "Epoch 195/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4269 - MSE: 5.4269 - val_loss: 5.2514 - val_MSE: 5.2514\n",
            "Epoch 196/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4153 - MSE: 5.4153 - val_loss: 5.2512 - val_MSE: 5.2512\n",
            "Epoch 197/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.4123 - MSE: 5.4123 - val_loss: 5.2038 - val_MSE: 5.2038\n",
            "Epoch 198/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3934 - MSE: 5.3934 - val_loss: 5.2501 - val_MSE: 5.2501\n",
            "Epoch 199/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3974 - MSE: 5.3974 - val_loss: 5.2162 - val_MSE: 5.2162\n",
            "Epoch 200/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3849 - MSE: 5.3849 - val_loss: 5.2286 - val_MSE: 5.2286\n",
            "Epoch 201/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3791 - MSE: 5.3791 - val_loss: 5.2335 - val_MSE: 5.2335\n",
            "Epoch 202/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3813 - MSE: 5.3813 - val_loss: 5.2216 - val_MSE: 5.2216\n",
            "Epoch 203/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3815 - MSE: 5.3815 - val_loss: 5.2061 - val_MSE: 5.2061\n",
            "Epoch 204/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3650 - MSE: 5.3650 - val_loss: 5.1967 - val_MSE: 5.1967\n",
            "Epoch 205/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3578 - MSE: 5.3578 - val_loss: 5.2022 - val_MSE: 5.2022\n",
            "Epoch 206/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3547 - MSE: 5.3547 - val_loss: 5.2405 - val_MSE: 5.2405\n",
            "Epoch 207/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3455 - MSE: 5.3455 - val_loss: 5.1683 - val_MSE: 5.1683\n",
            "Epoch 208/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3518 - MSE: 5.3518 - val_loss: 5.2619 - val_MSE: 5.2619\n",
            "Epoch 209/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3366 - MSE: 5.3366 - val_loss: 5.2304 - val_MSE: 5.2304\n",
            "Epoch 210/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3355 - MSE: 5.3355 - val_loss: 5.1594 - val_MSE: 5.1594\n",
            "Epoch 211/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3269 - MSE: 5.3269 - val_loss: 5.1647 - val_MSE: 5.1647\n",
            "Epoch 212/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3319 - MSE: 5.3319 - val_loss: 5.1579 - val_MSE: 5.1579\n",
            "Epoch 213/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3193 - MSE: 5.3193 - val_loss: 5.1467 - val_MSE: 5.1467\n",
            "Epoch 214/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.3123 - MSE: 5.3123 - val_loss: 5.1789 - val_MSE: 5.1789\n",
            "Epoch 215/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2939 - MSE: 5.2939 - val_loss: 5.1575 - val_MSE: 5.1575\n",
            "Epoch 216/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2961 - MSE: 5.2961 - val_loss: 5.1280 - val_MSE: 5.1280\n",
            "Epoch 217/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2915 - MSE: 5.2915 - val_loss: 5.0976 - val_MSE: 5.0976\n",
            "Epoch 218/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2796 - MSE: 5.2796 - val_loss: 5.1076 - val_MSE: 5.1076\n",
            "Epoch 219/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2821 - MSE: 5.2821 - val_loss: 5.1062 - val_MSE: 5.1062\n",
            "Epoch 220/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2755 - MSE: 5.2755 - val_loss: 5.1170 - val_MSE: 5.1170\n",
            "Epoch 221/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2716 - MSE: 5.2716 - val_loss: 5.0527 - val_MSE: 5.0527\n",
            "Epoch 222/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2678 - MSE: 5.2678 - val_loss: 5.0795 - val_MSE: 5.0795\n",
            "Epoch 223/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2644 - MSE: 5.2644 - val_loss: 5.2045 - val_MSE: 5.2045\n",
            "Epoch 224/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2539 - MSE: 5.2539 - val_loss: 5.0748 - val_MSE: 5.0748\n",
            "Epoch 225/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2575 - MSE: 5.2575 - val_loss: 5.1222 - val_MSE: 5.1222\n",
            "Epoch 226/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2428 - MSE: 5.2428 - val_loss: 5.0552 - val_MSE: 5.0552\n",
            "Epoch 227/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2461 - MSE: 5.2461 - val_loss: 5.1120 - val_MSE: 5.1120\n",
            "Epoch 228/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2371 - MSE: 5.2371 - val_loss: 5.1562 - val_MSE: 5.1562\n",
            "Epoch 229/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2320 - MSE: 5.2320 - val_loss: 5.0660 - val_MSE: 5.0660\n",
            "Epoch 230/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2247 - MSE: 5.2247 - val_loss: 5.0121 - val_MSE: 5.0121\n",
            "Epoch 231/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2205 - MSE: 5.2205 - val_loss: 5.0351 - val_MSE: 5.0351\n",
            "Epoch 232/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2159 - MSE: 5.2159 - val_loss: 4.9928 - val_MSE: 4.9928\n",
            "Epoch 233/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2100 - MSE: 5.2100 - val_loss: 5.0796 - val_MSE: 5.0796\n",
            "Epoch 234/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2104 - MSE: 5.2104 - val_loss: 5.0319 - val_MSE: 5.0319\n",
            "Epoch 235/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2089 - MSE: 5.2089 - val_loss: 5.0438 - val_MSE: 5.0438\n",
            "Epoch 236/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2071 - MSE: 5.2071 - val_loss: 5.0320 - val_MSE: 5.0320\n",
            "Epoch 237/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.2080 - MSE: 5.2080 - val_loss: 5.0091 - val_MSE: 5.0091\n",
            "Epoch 238/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1870 - MSE: 5.1870 - val_loss: 5.0003 - val_MSE: 5.0003\n",
            "Epoch 239/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1919 - MSE: 5.1919 - val_loss: 5.0234 - val_MSE: 5.0234\n",
            "Epoch 240/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1777 - MSE: 5.1777 - val_loss: 5.0311 - val_MSE: 5.0311\n",
            "Epoch 241/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1801 - MSE: 5.1801 - val_loss: 5.0739 - val_MSE: 5.0739\n",
            "Epoch 242/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1828 - MSE: 5.1828 - val_loss: 5.0102 - val_MSE: 5.0102\n",
            "Epoch 243/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1752 - MSE: 5.1752 - val_loss: 4.9979 - val_MSE: 4.9979\n",
            "Epoch 244/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1663 - MSE: 5.1663 - val_loss: 4.9694 - val_MSE: 4.9694\n",
            "Epoch 245/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1674 - MSE: 5.1674 - val_loss: 4.9672 - val_MSE: 4.9672\n",
            "Epoch 246/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1661 - MSE: 5.1661 - val_loss: 4.9654 - val_MSE: 4.9654\n",
            "Epoch 247/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1608 - MSE: 5.1608 - val_loss: 4.9767 - val_MSE: 4.9767\n",
            "Epoch 248/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1548 - MSE: 5.1548 - val_loss: 4.9931 - val_MSE: 4.9931\n",
            "Epoch 249/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1515 - MSE: 5.1515 - val_loss: 4.9297 - val_MSE: 4.9297\n",
            "Epoch 250/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1432 - MSE: 5.1432 - val_loss: 4.9706 - val_MSE: 4.9706\n",
            "Epoch 251/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1420 - MSE: 5.1420 - val_loss: 5.0202 - val_MSE: 5.0202\n",
            "Epoch 252/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1465 - MSE: 5.1465 - val_loss: 5.0048 - val_MSE: 5.0048\n",
            "Epoch 253/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1390 - MSE: 5.1390 - val_loss: 5.1311 - val_MSE: 5.1311\n",
            "Epoch 254/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1374 - MSE: 5.1374 - val_loss: 5.0348 - val_MSE: 5.0348\n",
            "Epoch 255/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1301 - MSE: 5.1301 - val_loss: 4.9327 - val_MSE: 4.9327\n",
            "Epoch 256/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1209 - MSE: 5.1209 - val_loss: 5.0658 - val_MSE: 5.0658\n",
            "Epoch 257/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1191 - MSE: 5.1191 - val_loss: 4.9418 - val_MSE: 4.9418\n",
            "Epoch 258/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1328 - MSE: 5.1328 - val_loss: 4.9376 - val_MSE: 4.9376\n",
            "Epoch 259/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1141 - MSE: 5.1141 - val_loss: 5.1873 - val_MSE: 5.1873\n",
            "Epoch 260/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1190 - MSE: 5.1190 - val_loss: 4.9955 - val_MSE: 4.9955\n",
            "Epoch 261/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1135 - MSE: 5.1135 - val_loss: 4.9248 - val_MSE: 4.9248\n",
            "Epoch 262/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1137 - MSE: 5.1137 - val_loss: 4.9822 - val_MSE: 4.9822\n",
            "Epoch 263/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1099 - MSE: 5.1099 - val_loss: 4.9426 - val_MSE: 4.9426\n",
            "Epoch 264/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1049 - MSE: 5.1049 - val_loss: 4.9593 - val_MSE: 4.9593\n",
            "Epoch 265/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.1164 - MSE: 5.1164 - val_loss: 4.8981 - val_MSE: 4.8981\n",
            "Epoch 266/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0975 - MSE: 5.0975 - val_loss: 4.8919 - val_MSE: 4.8919\n",
            "Epoch 267/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0948 - MSE: 5.0948 - val_loss: 4.9208 - val_MSE: 4.9208\n",
            "Epoch 268/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0917 - MSE: 5.0917 - val_loss: 4.9173 - val_MSE: 4.9173\n",
            "Epoch 269/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0892 - MSE: 5.0892 - val_loss: 4.9906 - val_MSE: 4.9906\n",
            "Epoch 270/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0906 - MSE: 5.0906 - val_loss: 4.9281 - val_MSE: 4.9281\n",
            "Epoch 271/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0934 - MSE: 5.0934 - val_loss: 4.9211 - val_MSE: 4.9211\n",
            "Epoch 272/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0762 - MSE: 5.0762 - val_loss: 4.9472 - val_MSE: 4.9472\n",
            "Epoch 273/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0820 - MSE: 5.0820 - val_loss: 4.9204 - val_MSE: 4.9204\n",
            "Epoch 274/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0812 - MSE: 5.0812 - val_loss: 4.8734 - val_MSE: 4.8734\n",
            "Epoch 275/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0789 - MSE: 5.0789 - val_loss: 4.8754 - val_MSE: 4.8754\n",
            "Epoch 276/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0746 - MSE: 5.0746 - val_loss: 4.9031 - val_MSE: 4.9031\n",
            "Epoch 277/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0710 - MSE: 5.0710 - val_loss: 4.9079 - val_MSE: 4.9079\n",
            "Epoch 278/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0759 - MSE: 5.0759 - val_loss: 4.9523 - val_MSE: 4.9523\n",
            "Epoch 279/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0754 - MSE: 5.0754 - val_loss: 4.8519 - val_MSE: 4.8519\n",
            "Epoch 280/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0745 - MSE: 5.0745 - val_loss: 4.8949 - val_MSE: 4.8949\n",
            "Epoch 281/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0625 - MSE: 5.0625 - val_loss: 4.9369 - val_MSE: 4.9369\n",
            "Epoch 282/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0621 - MSE: 5.0621 - val_loss: 4.9294 - val_MSE: 4.9294\n",
            "Epoch 283/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0570 - MSE: 5.0570 - val_loss: 4.9162 - val_MSE: 4.9162\n",
            "Epoch 284/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0498 - MSE: 5.0498 - val_loss: 4.8596 - val_MSE: 4.8596\n",
            "Epoch 285/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0514 - MSE: 5.0514 - val_loss: 5.0770 - val_MSE: 5.0770\n",
            "Epoch 286/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0542 - MSE: 5.0542 - val_loss: 4.9092 - val_MSE: 4.9092\n",
            "Epoch 287/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0607 - MSE: 5.0607 - val_loss: 4.8492 - val_MSE: 4.8492\n",
            "Epoch 288/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0353 - MSE: 5.0353 - val_loss: 4.9533 - val_MSE: 4.9533\n",
            "Epoch 289/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0416 - MSE: 5.0416 - val_loss: 4.9635 - val_MSE: 4.9635\n",
            "Epoch 290/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0371 - MSE: 5.0371 - val_loss: 4.9476 - val_MSE: 4.9476\n",
            "Epoch 291/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0401 - MSE: 5.0401 - val_loss: 4.8954 - val_MSE: 4.8954\n",
            "Epoch 292/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0381 - MSE: 5.0381 - val_loss: 4.8601 - val_MSE: 4.8601\n",
            "Epoch 293/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0281 - MSE: 5.0281 - val_loss: 4.8361 - val_MSE: 4.8361\n",
            "Epoch 294/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0281 - MSE: 5.0281 - val_loss: 4.8312 - val_MSE: 4.8312\n",
            "Epoch 295/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0364 - MSE: 5.0364 - val_loss: 4.8557 - val_MSE: 4.8557\n",
            "Epoch 296/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0352 - MSE: 5.0352 - val_loss: 4.8650 - val_MSE: 4.8650\n",
            "Epoch 297/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0251 - MSE: 5.0251 - val_loss: 4.8351 - val_MSE: 4.8351\n",
            "Epoch 298/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0212 - MSE: 5.0212 - val_loss: 4.8998 - val_MSE: 4.8998\n",
            "Epoch 299/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0124 - MSE: 5.0124 - val_loss: 4.9318 - val_MSE: 4.9318\n",
            "Epoch 300/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0144 - MSE: 5.0144 - val_loss: 4.8931 - val_MSE: 4.8931\n",
            "Epoch 301/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0071 - MSE: 5.0071 - val_loss: 4.9431 - val_MSE: 4.9431\n",
            "Epoch 302/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0160 - MSE: 5.0160 - val_loss: 4.8795 - val_MSE: 4.8795\n",
            "Epoch 303/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0136 - MSE: 5.0136 - val_loss: 4.9037 - val_MSE: 4.9037\n",
            "Epoch 304/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0137 - MSE: 5.0137 - val_loss: 4.8525 - val_MSE: 4.8525\n",
            "Epoch 305/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0023 - MSE: 5.0023 - val_loss: 4.8734 - val_MSE: 4.8734\n",
            "Epoch 306/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 5.0094 - MSE: 5.0094 - val_loss: 4.8290 - val_MSE: 4.8290\n",
            "Epoch 307/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9940 - MSE: 4.9940 - val_loss: 4.8016 - val_MSE: 4.8016\n",
            "Epoch 308/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 5.0036 - MSE: 5.0036 - val_loss: 4.8233 - val_MSE: 4.8233\n",
            "Epoch 309/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9938 - MSE: 4.9938 - val_loss: 4.8137 - val_MSE: 4.8137\n",
            "Epoch 310/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9892 - MSE: 4.9892 - val_loss: 4.8135 - val_MSE: 4.8135\n",
            "Epoch 311/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.9915 - MSE: 4.9915 - val_loss: 4.8198 - val_MSE: 4.8198\n",
            "Epoch 312/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9839 - MSE: 4.9839 - val_loss: 4.8248 - val_MSE: 4.8248\n",
            "Epoch 313/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9939 - MSE: 4.9939 - val_loss: 4.7988 - val_MSE: 4.7988\n",
            "Epoch 314/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9803 - MSE: 4.9803 - val_loss: 4.8001 - val_MSE: 4.8001\n",
            "Epoch 315/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9708 - MSE: 4.9708 - val_loss: 4.8496 - val_MSE: 4.8496\n",
            "Epoch 316/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9882 - MSE: 4.9882 - val_loss: 4.7996 - val_MSE: 4.7996\n",
            "Epoch 317/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9788 - MSE: 4.9788 - val_loss: 4.8265 - val_MSE: 4.8265\n",
            "Epoch 318/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9792 - MSE: 4.9792 - val_loss: 4.8344 - val_MSE: 4.8344\n",
            "Epoch 319/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.9640 - MSE: 4.9640 - val_loss: 4.7842 - val_MSE: 4.7842\n",
            "Epoch 320/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9634 - MSE: 4.9634 - val_loss: 4.8021 - val_MSE: 4.8021\n",
            "Epoch 321/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9800 - MSE: 4.9800 - val_loss: 4.7715 - val_MSE: 4.7715\n",
            "Epoch 322/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9666 - MSE: 4.9666 - val_loss: 4.8815 - val_MSE: 4.8815\n",
            "Epoch 323/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9524 - MSE: 4.9524 - val_loss: 4.8602 - val_MSE: 4.8602\n",
            "Epoch 324/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9551 - MSE: 4.9551 - val_loss: 4.8112 - val_MSE: 4.8112\n",
            "Epoch 325/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9601 - MSE: 4.9601 - val_loss: 4.7787 - val_MSE: 4.7787\n",
            "Epoch 326/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9527 - MSE: 4.9527 - val_loss: 4.8137 - val_MSE: 4.8137\n",
            "Epoch 327/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.9508 - MSE: 4.9508 - val_loss: 4.8499 - val_MSE: 4.8499\n",
            "Epoch 328/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9408 - MSE: 4.9408 - val_loss: 4.8097 - val_MSE: 4.8097\n",
            "Epoch 329/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9494 - MSE: 4.9494 - val_loss: 4.7596 - val_MSE: 4.7596\n",
            "Epoch 330/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9410 - MSE: 4.9410 - val_loss: 4.7613 - val_MSE: 4.7613\n",
            "Epoch 331/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9379 - MSE: 4.9379 - val_loss: 4.8350 - val_MSE: 4.8350\n",
            "Epoch 332/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.9400 - MSE: 4.9400 - val_loss: 4.7407 - val_MSE: 4.7407\n",
            "Epoch 333/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9273 - MSE: 4.9273 - val_loss: 4.7366 - val_MSE: 4.7366\n",
            "Epoch 334/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.9234 - MSE: 4.9234 - val_loss: 4.7378 - val_MSE: 4.7378\n",
            "Epoch 335/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9179 - MSE: 4.9179 - val_loss: 4.8806 - val_MSE: 4.8806\n",
            "Epoch 336/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9250 - MSE: 4.9250 - val_loss: 4.7613 - val_MSE: 4.7613\n",
            "Epoch 337/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9119 - MSE: 4.9119 - val_loss: 4.7392 - val_MSE: 4.7392\n",
            "Epoch 338/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9081 - MSE: 4.9081 - val_loss: 4.7617 - val_MSE: 4.7617\n",
            "Epoch 339/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9081 - MSE: 4.9081 - val_loss: 4.7519 - val_MSE: 4.7519\n",
            "Epoch 340/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9093 - MSE: 4.9093 - val_loss: 4.7403 - val_MSE: 4.7403\n",
            "Epoch 341/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9009 - MSE: 4.9009 - val_loss: 4.7832 - val_MSE: 4.7832\n",
            "Epoch 342/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8950 - MSE: 4.8950 - val_loss: 4.7365 - val_MSE: 4.7365\n",
            "Epoch 343/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9067 - MSE: 4.9067 - val_loss: 4.7940 - val_MSE: 4.7940\n",
            "Epoch 344/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8906 - MSE: 4.8906 - val_loss: 4.8188 - val_MSE: 4.8188\n",
            "Epoch 345/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.9001 - MSE: 4.9001 - val_loss: 4.7385 - val_MSE: 4.7385\n",
            "Epoch 346/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8934 - MSE: 4.8934 - val_loss: 4.7574 - val_MSE: 4.7574\n",
            "Epoch 347/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8943 - MSE: 4.8943 - val_loss: 4.6953 - val_MSE: 4.6953\n",
            "Epoch 348/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8839 - MSE: 4.8839 - val_loss: 4.7367 - val_MSE: 4.7367\n",
            "Epoch 349/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.8786 - MSE: 4.8786 - val_loss: 4.7929 - val_MSE: 4.7929\n",
            "Epoch 350/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8678 - MSE: 4.8678 - val_loss: 4.8189 - val_MSE: 4.8189\n",
            "Epoch 351/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.8743 - MSE: 4.8743 - val_loss: 4.6850 - val_MSE: 4.6850\n",
            "Epoch 352/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.8566 - MSE: 4.8566 - val_loss: 4.8007 - val_MSE: 4.8007\n",
            "Epoch 353/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.8644 - MSE: 4.8644 - val_loss: 4.7860 - val_MSE: 4.7860\n",
            "Epoch 354/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.8554 - MSE: 4.8554 - val_loss: 4.7332 - val_MSE: 4.7332\n",
            "Epoch 355/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8639 - MSE: 4.8639 - val_loss: 4.7132 - val_MSE: 4.7132\n",
            "Epoch 356/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8539 - MSE: 4.8539 - val_loss: 4.6869 - val_MSE: 4.6869\n",
            "Epoch 357/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8611 - MSE: 4.8611 - val_loss: 4.7914 - val_MSE: 4.7914\n",
            "Epoch 358/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.8475 - MSE: 4.8475 - val_loss: 4.7623 - val_MSE: 4.7623\n",
            "Epoch 359/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8384 - MSE: 4.8384 - val_loss: 4.6697 - val_MSE: 4.6697\n",
            "Epoch 360/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8302 - MSE: 4.8302 - val_loss: 4.6886 - val_MSE: 4.6886\n",
            "Epoch 361/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8346 - MSE: 4.8346 - val_loss: 4.7202 - val_MSE: 4.7202\n",
            "Epoch 362/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8338 - MSE: 4.8338 - val_loss: 4.6771 - val_MSE: 4.6771\n",
            "Epoch 363/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8307 - MSE: 4.8307 - val_loss: 4.6636 - val_MSE: 4.6636\n",
            "Epoch 364/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8221 - MSE: 4.8221 - val_loss: 4.7005 - val_MSE: 4.7005\n",
            "Epoch 365/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8140 - MSE: 4.8140 - val_loss: 4.7001 - val_MSE: 4.7001\n",
            "Epoch 366/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8113 - MSE: 4.8113 - val_loss: 4.6533 - val_MSE: 4.6533\n",
            "Epoch 367/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.8158 - MSE: 4.8158 - val_loss: 4.6931 - val_MSE: 4.6931\n",
            "Epoch 368/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8051 - MSE: 4.8051 - val_loss: 4.6565 - val_MSE: 4.6565\n",
            "Epoch 369/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.8076 - MSE: 4.8076 - val_loss: 4.7201 - val_MSE: 4.7201\n",
            "Epoch 370/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7955 - MSE: 4.7955 - val_loss: 4.6385 - val_MSE: 4.6385\n",
            "Epoch 371/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7920 - MSE: 4.7920 - val_loss: 4.7182 - val_MSE: 4.7182\n",
            "Epoch 372/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7951 - MSE: 4.7951 - val_loss: 4.6581 - val_MSE: 4.6581\n",
            "Epoch 373/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7970 - MSE: 4.7970 - val_loss: 4.6390 - val_MSE: 4.6390\n",
            "Epoch 374/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7821 - MSE: 4.7821 - val_loss: 4.6772 - val_MSE: 4.6772\n",
            "Epoch 375/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7848 - MSE: 4.7848 - val_loss: 4.6450 - val_MSE: 4.6450\n",
            "Epoch 376/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7856 - MSE: 4.7856 - val_loss: 4.6614 - val_MSE: 4.6614\n",
            "Epoch 377/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7835 - MSE: 4.7835 - val_loss: 4.6832 - val_MSE: 4.6832\n",
            "Epoch 378/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7733 - MSE: 4.7733 - val_loss: 4.6373 - val_MSE: 4.6373\n",
            "Epoch 379/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7677 - MSE: 4.7677 - val_loss: 4.6371 - val_MSE: 4.6371\n",
            "Epoch 380/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7666 - MSE: 4.7666 - val_loss: 4.6427 - val_MSE: 4.6427\n",
            "Epoch 381/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7594 - MSE: 4.7594 - val_loss: 4.6090 - val_MSE: 4.6090\n",
            "Epoch 382/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7582 - MSE: 4.7582 - val_loss: 4.6300 - val_MSE: 4.6300\n",
            "Epoch 383/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7565 - MSE: 4.7565 - val_loss: 4.6107 - val_MSE: 4.6107\n",
            "Epoch 384/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7570 - MSE: 4.7570 - val_loss: 4.6941 - val_MSE: 4.6941\n",
            "Epoch 385/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7559 - MSE: 4.7559 - val_loss: 4.6368 - val_MSE: 4.6368\n",
            "Epoch 386/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7447 - MSE: 4.7447 - val_loss: 4.6830 - val_MSE: 4.6830\n",
            "Epoch 387/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7484 - MSE: 4.7484 - val_loss: 4.6567 - val_MSE: 4.6567\n",
            "Epoch 388/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7338 - MSE: 4.7338 - val_loss: 4.5993 - val_MSE: 4.5993\n",
            "Epoch 389/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7405 - MSE: 4.7405 - val_loss: 4.8368 - val_MSE: 4.8368\n",
            "Epoch 390/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7287 - MSE: 4.7287 - val_loss: 4.6506 - val_MSE: 4.6506\n",
            "Epoch 391/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7279 - MSE: 4.7279 - val_loss: 4.6285 - val_MSE: 4.6285\n",
            "Epoch 392/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7300 - MSE: 4.7300 - val_loss: 4.6554 - val_MSE: 4.6554\n",
            "Epoch 393/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7304 - MSE: 4.7304 - val_loss: 4.6097 - val_MSE: 4.6097\n",
            "Epoch 394/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7242 - MSE: 4.7242 - val_loss: 4.6202 - val_MSE: 4.6202\n",
            "Epoch 395/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.7211 - MSE: 4.7211 - val_loss: 4.6460 - val_MSE: 4.6460\n",
            "Epoch 396/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7226 - MSE: 4.7226 - val_loss: 4.6257 - val_MSE: 4.6257\n",
            "Epoch 397/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7118 - MSE: 4.7118 - val_loss: 4.5833 - val_MSE: 4.5833\n",
            "Epoch 398/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7145 - MSE: 4.7145 - val_loss: 4.5823 - val_MSE: 4.5823\n",
            "Epoch 399/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7094 - MSE: 4.7094 - val_loss: 4.5967 - val_MSE: 4.5967\n",
            "Epoch 400/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7152 - MSE: 4.7152 - val_loss: 4.5742 - val_MSE: 4.5742\n",
            "Epoch 401/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7061 - MSE: 4.7061 - val_loss: 4.5506 - val_MSE: 4.5506\n",
            "Epoch 402/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6988 - MSE: 4.6988 - val_loss: 4.6320 - val_MSE: 4.6320\n",
            "Epoch 403/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.7006 - MSE: 4.7006 - val_loss: 4.6268 - val_MSE: 4.6268\n",
            "Epoch 404/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6944 - MSE: 4.6944 - val_loss: 4.5714 - val_MSE: 4.5714\n",
            "Epoch 405/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6969 - MSE: 4.6969 - val_loss: 4.5946 - val_MSE: 4.5946\n",
            "Epoch 406/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6876 - MSE: 4.6876 - val_loss: 4.7281 - val_MSE: 4.7281\n",
            "Epoch 407/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6864 - MSE: 4.6864 - val_loss: 4.5513 - val_MSE: 4.5513\n",
            "Epoch 408/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6901 - MSE: 4.6901 - val_loss: 4.5773 - val_MSE: 4.5773\n",
            "Epoch 409/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6783 - MSE: 4.6783 - val_loss: 4.7111 - val_MSE: 4.7111\n",
            "Epoch 410/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6823 - MSE: 4.6823 - val_loss: 4.6047 - val_MSE: 4.6047\n",
            "Epoch 411/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6809 - MSE: 4.6809 - val_loss: 4.5421 - val_MSE: 4.5421\n",
            "Epoch 412/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6770 - MSE: 4.6770 - val_loss: 4.7058 - val_MSE: 4.7058\n",
            "Epoch 413/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6649 - MSE: 4.6649 - val_loss: 4.6732 - val_MSE: 4.6732\n",
            "Epoch 414/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6683 - MSE: 4.6683 - val_loss: 4.5607 - val_MSE: 4.5607\n",
            "Epoch 415/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6739 - MSE: 4.6739 - val_loss: 4.5842 - val_MSE: 4.5842\n",
            "Epoch 416/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6648 - MSE: 4.6648 - val_loss: 4.5854 - val_MSE: 4.5854\n",
            "Epoch 417/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6547 - MSE: 4.6547 - val_loss: 4.5829 - val_MSE: 4.5829\n",
            "Epoch 418/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6524 - MSE: 4.6524 - val_loss: 4.5812 - val_MSE: 4.5812\n",
            "Epoch 419/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6524 - MSE: 4.6524 - val_loss: 4.5641 - val_MSE: 4.5641\n",
            "Epoch 420/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6600 - MSE: 4.6600 - val_loss: 4.6254 - val_MSE: 4.6254\n",
            "Epoch 421/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6441 - MSE: 4.6441 - val_loss: 4.6655 - val_MSE: 4.6655\n",
            "Epoch 422/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6550 - MSE: 4.6550 - val_loss: 4.5624 - val_MSE: 4.5624\n",
            "Epoch 423/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6500 - MSE: 4.6500 - val_loss: 4.6079 - val_MSE: 4.6079\n",
            "Epoch 424/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6378 - MSE: 4.6378 - val_loss: 4.5298 - val_MSE: 4.5298\n",
            "Epoch 425/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6457 - MSE: 4.6457 - val_loss: 4.5592 - val_MSE: 4.5592\n",
            "Epoch 426/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6415 - MSE: 4.6415 - val_loss: 4.5409 - val_MSE: 4.5409\n",
            "Epoch 427/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6334 - MSE: 4.6334 - val_loss: 4.5797 - val_MSE: 4.5797\n",
            "Epoch 428/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6245 - MSE: 4.6245 - val_loss: 4.5383 - val_MSE: 4.5383\n",
            "Epoch 429/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.6381 - MSE: 4.6381 - val_loss: 4.5084 - val_MSE: 4.5084\n",
            "Epoch 430/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6355 - MSE: 4.6355 - val_loss: 4.5501 - val_MSE: 4.5501\n",
            "Epoch 431/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6316 - MSE: 4.6316 - val_loss: 4.5737 - val_MSE: 4.5737\n",
            "Epoch 432/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6208 - MSE: 4.6208 - val_loss: 4.5409 - val_MSE: 4.5409\n",
            "Epoch 433/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6224 - MSE: 4.6224 - val_loss: 4.5341 - val_MSE: 4.5341\n",
            "Epoch 434/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6230 - MSE: 4.6230 - val_loss: 4.6280 - val_MSE: 4.6280\n",
            "Epoch 435/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6106 - MSE: 4.6106 - val_loss: 4.5671 - val_MSE: 4.5671\n",
            "Epoch 436/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6073 - MSE: 4.6073 - val_loss: 4.4965 - val_MSE: 4.4965\n",
            "Epoch 437/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6073 - MSE: 4.6073 - val_loss: 4.5087 - val_MSE: 4.5087\n",
            "Epoch 438/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6012 - MSE: 4.6012 - val_loss: 4.5260 - val_MSE: 4.5260\n",
            "Epoch 439/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.6117 - MSE: 4.6117 - val_loss: 4.4924 - val_MSE: 4.4924\n",
            "Epoch 440/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.6012 - MSE: 4.6012 - val_loss: 4.5097 - val_MSE: 4.5097\n",
            "Epoch 441/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5985 - MSE: 4.5985 - val_loss: 4.5101 - val_MSE: 4.5101\n",
            "Epoch 442/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5940 - MSE: 4.5940 - val_loss: 4.5441 - val_MSE: 4.5441\n",
            "Epoch 443/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5954 - MSE: 4.5954 - val_loss: 4.5097 - val_MSE: 4.5097\n",
            "Epoch 444/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5918 - MSE: 4.5918 - val_loss: 4.5274 - val_MSE: 4.5274\n",
            "Epoch 445/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5909 - MSE: 4.5909 - val_loss: 4.4896 - val_MSE: 4.4896\n",
            "Epoch 446/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5762 - MSE: 4.5762 - val_loss: 4.5234 - val_MSE: 4.5234\n",
            "Epoch 447/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5973 - MSE: 4.5973 - val_loss: 4.5617 - val_MSE: 4.5617\n",
            "Epoch 448/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5926 - MSE: 4.5926 - val_loss: 4.4811 - val_MSE: 4.4811\n",
            "Epoch 449/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5787 - MSE: 4.5787 - val_loss: 4.5429 - val_MSE: 4.5429\n",
            "Epoch 450/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5744 - MSE: 4.5744 - val_loss: 4.4761 - val_MSE: 4.4761\n",
            "Epoch 451/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5873 - MSE: 4.5873 - val_loss: 4.4874 - val_MSE: 4.4874\n",
            "Epoch 452/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5814 - MSE: 4.5814 - val_loss: 4.4789 - val_MSE: 4.4789\n",
            "Epoch 453/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5800 - MSE: 4.5800 - val_loss: 4.5388 - val_MSE: 4.5388\n",
            "Epoch 454/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5647 - MSE: 4.5647 - val_loss: 4.5485 - val_MSE: 4.5485\n",
            "Epoch 455/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5703 - MSE: 4.5703 - val_loss: 4.4934 - val_MSE: 4.4935\n",
            "Epoch 456/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5736 - MSE: 4.5736 - val_loss: 4.4537 - val_MSE: 4.4537\n",
            "Epoch 457/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5569 - MSE: 4.5569 - val_loss: 4.5580 - val_MSE: 4.5580\n",
            "Epoch 458/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5663 - MSE: 4.5663 - val_loss: 4.4896 - val_MSE: 4.4896\n",
            "Epoch 459/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5647 - MSE: 4.5647 - val_loss: 4.4585 - val_MSE: 4.4585\n",
            "Epoch 460/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5665 - MSE: 4.5665 - val_loss: 4.4785 - val_MSE: 4.4785\n",
            "Epoch 461/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5465 - MSE: 4.5465 - val_loss: 4.5050 - val_MSE: 4.5050\n",
            "Epoch 462/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5475 - MSE: 4.5475 - val_loss: 4.5197 - val_MSE: 4.5197\n",
            "Epoch 463/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5527 - MSE: 4.5527 - val_loss: 4.5031 - val_MSE: 4.5031\n",
            "Epoch 464/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5544 - MSE: 4.5544 - val_loss: 4.4666 - val_MSE: 4.4666\n",
            "Epoch 465/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5437 - MSE: 4.5437 - val_loss: 4.4832 - val_MSE: 4.4832\n",
            "Epoch 466/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5526 - MSE: 4.5526 - val_loss: 4.5569 - val_MSE: 4.5569\n",
            "Epoch 467/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5522 - MSE: 4.5522 - val_loss: 4.4444 - val_MSE: 4.4444\n",
            "Epoch 468/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5462 - MSE: 4.5462 - val_loss: 4.4881 - val_MSE: 4.4881\n",
            "Epoch 469/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5433 - MSE: 4.5433 - val_loss: 4.4485 - val_MSE: 4.4485\n",
            "Epoch 470/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5429 - MSE: 4.5429 - val_loss: 4.4550 - val_MSE: 4.4550\n",
            "Epoch 471/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5270 - MSE: 4.5270 - val_loss: 4.4946 - val_MSE: 4.4946\n",
            "Epoch 472/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5333 - MSE: 4.5333 - val_loss: 4.5097 - val_MSE: 4.5097\n",
            "Epoch 473/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5298 - MSE: 4.5298 - val_loss: 4.4634 - val_MSE: 4.4634\n",
            "Epoch 474/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5400 - MSE: 4.5400 - val_loss: 4.4818 - val_MSE: 4.4818\n",
            "Epoch 475/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5342 - MSE: 4.5342 - val_loss: 4.5544 - val_MSE: 4.5544\n",
            "Epoch 476/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5274 - MSE: 4.5274 - val_loss: 4.5036 - val_MSE: 4.5036\n",
            "Epoch 477/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5232 - MSE: 4.5232 - val_loss: 4.4847 - val_MSE: 4.4847\n",
            "Epoch 478/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5252 - MSE: 4.5252 - val_loss: 4.4167 - val_MSE: 4.4167\n",
            "Epoch 479/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5261 - MSE: 4.5261 - val_loss: 4.5615 - val_MSE: 4.5615\n",
            "Epoch 480/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5154 - MSE: 4.5154 - val_loss: 4.4595 - val_MSE: 4.4595\n",
            "Epoch 481/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5166 - MSE: 4.5166 - val_loss: 4.4864 - val_MSE: 4.4864\n",
            "Epoch 482/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5174 - MSE: 4.5174 - val_loss: 4.4845 - val_MSE: 4.4845\n",
            "Epoch 483/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5113 - MSE: 4.5113 - val_loss: 4.4182 - val_MSE: 4.4182\n",
            "Epoch 484/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5129 - MSE: 4.5129 - val_loss: 4.4474 - val_MSE: 4.4474\n",
            "Epoch 485/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5050 - MSE: 4.5050 - val_loss: 4.4611 - val_MSE: 4.4611\n",
            "Epoch 486/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5103 - MSE: 4.5103 - val_loss: 4.4603 - val_MSE: 4.4603\n",
            "Epoch 487/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5125 - MSE: 4.5125 - val_loss: 4.4216 - val_MSE: 4.4216\n",
            "Epoch 488/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5017 - MSE: 4.5017 - val_loss: 4.5611 - val_MSE: 4.5611\n",
            "Epoch 489/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.5168 - MSE: 4.5168 - val_loss: 4.4484 - val_MSE: 4.4484\n",
            "Epoch 490/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5089 - MSE: 4.5089 - val_loss: 4.4034 - val_MSE: 4.4034\n",
            "Epoch 491/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.5019 - MSE: 4.5019 - val_loss: 4.4308 - val_MSE: 4.4308\n",
            "Epoch 492/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4966 - MSE: 4.4966 - val_loss: 4.4486 - val_MSE: 4.4486\n",
            "Epoch 493/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4829 - MSE: 4.4829 - val_loss: 4.4325 - val_MSE: 4.4325\n",
            "Epoch 494/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4938 - MSE: 4.4938 - val_loss: 4.4479 - val_MSE: 4.4479\n",
            "Epoch 495/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4855 - MSE: 4.4855 - val_loss: 4.4768 - val_MSE: 4.4768\n",
            "Epoch 496/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4968 - MSE: 4.4968 - val_loss: 4.4492 - val_MSE: 4.4492\n",
            "Epoch 497/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4908 - MSE: 4.4908 - val_loss: 4.4807 - val_MSE: 4.4807\n",
            "Epoch 498/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4869 - MSE: 4.4869 - val_loss: 4.3842 - val_MSE: 4.3842\n",
            "Epoch 499/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4863 - MSE: 4.4863 - val_loss: 4.4049 - val_MSE: 4.4049\n",
            "Epoch 500/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4804 - MSE: 4.4804 - val_loss: 4.4789 - val_MSE: 4.4789\n",
            "Epoch 501/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4851 - MSE: 4.4851 - val_loss: 4.3919 - val_MSE: 4.3919\n",
            "Epoch 502/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4789 - MSE: 4.4789 - val_loss: 4.4267 - val_MSE: 4.4267\n",
            "Epoch 503/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4769 - MSE: 4.4769 - val_loss: 4.4662 - val_MSE: 4.4662\n",
            "Epoch 504/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4771 - MSE: 4.4771 - val_loss: 4.3952 - val_MSE: 4.3952\n",
            "Epoch 505/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4802 - MSE: 4.4802 - val_loss: 4.4452 - val_MSE: 4.4452\n",
            "Epoch 506/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4775 - MSE: 4.4775 - val_loss: 4.4591 - val_MSE: 4.4591\n",
            "Epoch 507/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4811 - MSE: 4.4811 - val_loss: 4.4732 - val_MSE: 4.4732\n",
            "Epoch 508/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4748 - MSE: 4.4748 - val_loss: 4.4287 - val_MSE: 4.4287\n",
            "Epoch 509/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4791 - MSE: 4.4791 - val_loss: 4.4155 - val_MSE: 4.4155\n",
            "Epoch 510/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4689 - MSE: 4.4689 - val_loss: 4.3827 - val_MSE: 4.3827\n",
            "Epoch 511/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4702 - MSE: 4.4702 - val_loss: 4.4034 - val_MSE: 4.4034\n",
            "Epoch 512/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4636 - MSE: 4.4636 - val_loss: 4.4191 - val_MSE: 4.4191\n",
            "Epoch 513/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4672 - MSE: 4.4672 - val_loss: 4.3859 - val_MSE: 4.3859\n",
            "Epoch 514/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4600 - MSE: 4.4600 - val_loss: 4.4225 - val_MSE: 4.4225\n",
            "Epoch 515/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4563 - MSE: 4.4563 - val_loss: 4.4215 - val_MSE: 4.4215\n",
            "Epoch 516/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4714 - MSE: 4.4714 - val_loss: 4.3676 - val_MSE: 4.3676\n",
            "Epoch 517/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4516 - MSE: 4.4516 - val_loss: 4.3579 - val_MSE: 4.3579\n",
            "Epoch 518/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4525 - MSE: 4.4525 - val_loss: 4.5470 - val_MSE: 4.5470\n",
            "Epoch 519/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4528 - MSE: 4.4528 - val_loss: 4.3845 - val_MSE: 4.3845\n",
            "Epoch 520/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4491 - MSE: 4.4491 - val_loss: 4.3605 - val_MSE: 4.3605\n",
            "Epoch 521/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4485 - MSE: 4.4485 - val_loss: 4.3886 - val_MSE: 4.3886\n",
            "Epoch 522/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4440 - MSE: 4.4440 - val_loss: 4.4140 - val_MSE: 4.4140\n",
            "Epoch 523/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4538 - MSE: 4.4538 - val_loss: 4.3923 - val_MSE: 4.3923\n",
            "Epoch 524/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4469 - MSE: 4.4469 - val_loss: 4.3954 - val_MSE: 4.3954\n",
            "Epoch 525/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4449 - MSE: 4.4449 - val_loss: 4.3559 - val_MSE: 4.3559\n",
            "Epoch 526/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4411 - MSE: 4.4411 - val_loss: 4.4187 - val_MSE: 4.4187\n",
            "Epoch 527/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4425 - MSE: 4.4425 - val_loss: 4.4226 - val_MSE: 4.4226\n",
            "Epoch 528/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4346 - MSE: 4.4346 - val_loss: 4.3974 - val_MSE: 4.3974\n",
            "Epoch 529/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4441 - MSE: 4.4441 - val_loss: 4.3526 - val_MSE: 4.3526\n",
            "Epoch 530/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4395 - MSE: 4.4395 - val_loss: 4.3733 - val_MSE: 4.3733\n",
            "Epoch 531/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4389 - MSE: 4.4389 - val_loss: 4.3417 - val_MSE: 4.3417\n",
            "Epoch 532/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4332 - MSE: 4.4332 - val_loss: 4.4090 - val_MSE: 4.4090\n",
            "Epoch 533/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4270 - MSE: 4.4270 - val_loss: 4.4378 - val_MSE: 4.4378\n",
            "Epoch 534/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4273 - MSE: 4.4273 - val_loss: 4.4563 - val_MSE: 4.4563\n",
            "Epoch 535/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4356 - MSE: 4.4356 - val_loss: 4.3606 - val_MSE: 4.3606\n",
            "Epoch 536/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4273 - MSE: 4.4273 - val_loss: 4.4170 - val_MSE: 4.4170\n",
            "Epoch 537/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4285 - MSE: 4.4285 - val_loss: 4.3660 - val_MSE: 4.3660\n",
            "Epoch 538/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.4190 - MSE: 4.4190 - val_loss: 4.3512 - val_MSE: 4.3512\n",
            "Epoch 539/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4274 - MSE: 4.4274 - val_loss: 4.3420 - val_MSE: 4.3420\n",
            "Epoch 540/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4306 - MSE: 4.4306 - val_loss: 4.3777 - val_MSE: 4.3777\n",
            "Epoch 541/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4117 - MSE: 4.4117 - val_loss: 4.4049 - val_MSE: 4.4049\n",
            "Epoch 542/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4247 - MSE: 4.4247 - val_loss: 4.4105 - val_MSE: 4.4105\n",
            "Epoch 543/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4161 - MSE: 4.4161 - val_loss: 4.3508 - val_MSE: 4.3508\n",
            "Epoch 544/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4136 - MSE: 4.4136 - val_loss: 4.4263 - val_MSE: 4.4263\n",
            "Epoch 545/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4133 - MSE: 4.4133 - val_loss: 4.3236 - val_MSE: 4.3236\n",
            "Epoch 546/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4079 - MSE: 4.4079 - val_loss: 4.3712 - val_MSE: 4.3712\n",
            "Epoch 547/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4126 - MSE: 4.4126 - val_loss: 4.3434 - val_MSE: 4.3434\n",
            "Epoch 548/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4024 - MSE: 4.4024 - val_loss: 4.3419 - val_MSE: 4.3419\n",
            "Epoch 549/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4067 - MSE: 4.4067 - val_loss: 4.3550 - val_MSE: 4.3550\n",
            "Epoch 550/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4043 - MSE: 4.4043 - val_loss: 4.3343 - val_MSE: 4.3343\n",
            "Epoch 551/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4060 - MSE: 4.4060 - val_loss: 4.3467 - val_MSE: 4.3467\n",
            "Epoch 552/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4125 - MSE: 4.4125 - val_loss: 4.3676 - val_MSE: 4.3676\n",
            "Epoch 553/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.4141 - MSE: 4.4141 - val_loss: 4.3932 - val_MSE: 4.3932\n",
            "Epoch 554/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3978 - MSE: 4.3978 - val_loss: 4.3165 - val_MSE: 4.3165\n",
            "Epoch 555/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3979 - MSE: 4.3979 - val_loss: 4.3082 - val_MSE: 4.3082\n",
            "Epoch 556/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3983 - MSE: 4.3983 - val_loss: 4.3072 - val_MSE: 4.3072\n",
            "Epoch 557/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3989 - MSE: 4.3989 - val_loss: 4.3263 - val_MSE: 4.3263\n",
            "Epoch 558/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3953 - MSE: 4.3953 - val_loss: 4.3510 - val_MSE: 4.3510\n",
            "Epoch 559/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3984 - MSE: 4.3984 - val_loss: 4.3689 - val_MSE: 4.3689\n",
            "Epoch 560/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3918 - MSE: 4.3918 - val_loss: 4.3257 - val_MSE: 4.3257\n",
            "Epoch 561/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3836 - MSE: 4.3836 - val_loss: 4.3500 - val_MSE: 4.3500\n",
            "Epoch 562/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3932 - MSE: 4.3932 - val_loss: 4.3268 - val_MSE: 4.3268\n",
            "Epoch 563/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3890 - MSE: 4.3890 - val_loss: 4.3015 - val_MSE: 4.3015\n",
            "Epoch 564/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3877 - MSE: 4.3877 - val_loss: 4.3267 - val_MSE: 4.3267\n",
            "Epoch 565/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.3892 - MSE: 4.3892 - val_loss: 4.3230 - val_MSE: 4.3230\n",
            "Epoch 566/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3826 - MSE: 4.3826 - val_loss: 4.3687 - val_MSE: 4.3687\n",
            "Epoch 567/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3679 - MSE: 4.3679 - val_loss: 4.4523 - val_MSE: 4.4523\n",
            "Epoch 568/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3777 - MSE: 4.3777 - val_loss: 4.3352 - val_MSE: 4.3352\n",
            "Epoch 569/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3768 - MSE: 4.3768 - val_loss: 4.3315 - val_MSE: 4.3315\n",
            "Epoch 570/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3767 - MSE: 4.3767 - val_loss: 4.2994 - val_MSE: 4.2994\n",
            "Epoch 571/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3845 - MSE: 4.3845 - val_loss: 4.3327 - val_MSE: 4.3327\n",
            "Epoch 572/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3807 - MSE: 4.3807 - val_loss: 4.2876 - val_MSE: 4.2876\n",
            "Epoch 573/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3751 - MSE: 4.3751 - val_loss: 4.3564 - val_MSE: 4.3564\n",
            "Epoch 574/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3735 - MSE: 4.3735 - val_loss: 4.3271 - val_MSE: 4.3271\n",
            "Epoch 575/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3777 - MSE: 4.3777 - val_loss: 4.3161 - val_MSE: 4.3161\n",
            "Epoch 576/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3620 - MSE: 4.3620 - val_loss: 4.3184 - val_MSE: 4.3184\n",
            "Epoch 577/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3685 - MSE: 4.3685 - val_loss: 4.3147 - val_MSE: 4.3147\n",
            "Epoch 578/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3672 - MSE: 4.3672 - val_loss: 4.3600 - val_MSE: 4.3600\n",
            "Epoch 579/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3744 - MSE: 4.3744 - val_loss: 4.3228 - val_MSE: 4.3228\n",
            "Epoch 580/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3595 - MSE: 4.3595 - val_loss: 4.3064 - val_MSE: 4.3064\n",
            "Epoch 581/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3608 - MSE: 4.3608 - val_loss: 4.3026 - val_MSE: 4.3026\n",
            "Epoch 582/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3682 - MSE: 4.3682 - val_loss: 4.3006 - val_MSE: 4.3006\n",
            "Epoch 583/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3666 - MSE: 4.3666 - val_loss: 4.2973 - val_MSE: 4.2973\n",
            "Epoch 584/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3619 - MSE: 4.3619 - val_loss: 4.3601 - val_MSE: 4.3601\n",
            "Epoch 585/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3512 - MSE: 4.3512 - val_loss: 4.2940 - val_MSE: 4.2940\n",
            "Epoch 586/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3533 - MSE: 4.3533 - val_loss: 4.3059 - val_MSE: 4.3059\n",
            "Epoch 587/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3523 - MSE: 4.3523 - val_loss: 4.2858 - val_MSE: 4.2858\n",
            "Epoch 588/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3519 - MSE: 4.3519 - val_loss: 4.3347 - val_MSE: 4.3347\n",
            "Epoch 589/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3470 - MSE: 4.3470 - val_loss: 4.3401 - val_MSE: 4.3401\n",
            "Epoch 590/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3463 - MSE: 4.3463 - val_loss: 4.2850 - val_MSE: 4.2850\n",
            "Epoch 591/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3498 - MSE: 4.3498 - val_loss: 4.2864 - val_MSE: 4.2864\n",
            "Epoch 592/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.3464 - MSE: 4.3464 - val_loss: 4.3279 - val_MSE: 4.3279\n",
            "Epoch 593/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3400 - MSE: 4.3400 - val_loss: 4.3153 - val_MSE: 4.3153\n",
            "Epoch 594/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3443 - MSE: 4.3443 - val_loss: 4.3809 - val_MSE: 4.3809\n",
            "Epoch 595/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3431 - MSE: 4.3431 - val_loss: 4.3253 - val_MSE: 4.3253\n",
            "Epoch 596/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3319 - MSE: 4.3319 - val_loss: 4.3321 - val_MSE: 4.3321\n",
            "Epoch 597/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3459 - MSE: 4.3459 - val_loss: 4.2965 - val_MSE: 4.2965\n",
            "Epoch 598/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3336 - MSE: 4.3336 - val_loss: 4.2819 - val_MSE: 4.2819\n",
            "Epoch 599/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3357 - MSE: 4.3357 - val_loss: 4.4329 - val_MSE: 4.4329\n",
            "Epoch 600/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3476 - MSE: 4.3476 - val_loss: 4.4127 - val_MSE: 4.4127\n",
            "Epoch 601/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3354 - MSE: 4.3354 - val_loss: 4.2508 - val_MSE: 4.2508\n",
            "Epoch 602/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3278 - MSE: 4.3278 - val_loss: 4.3835 - val_MSE: 4.3835\n",
            "Epoch 603/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3321 - MSE: 4.3321 - val_loss: 4.2616 - val_MSE: 4.2616\n",
            "Epoch 604/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3240 - MSE: 4.3240 - val_loss: 4.3367 - val_MSE: 4.3367\n",
            "Epoch 605/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3294 - MSE: 4.3294 - val_loss: 4.2697 - val_MSE: 4.2697\n",
            "Epoch 606/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3204 - MSE: 4.3204 - val_loss: 4.2577 - val_MSE: 4.2577\n",
            "Epoch 607/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3230 - MSE: 4.3230 - val_loss: 4.2652 - val_MSE: 4.2652\n",
            "Epoch 608/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3196 - MSE: 4.3196 - val_loss: 4.2949 - val_MSE: 4.2949\n",
            "Epoch 609/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3231 - MSE: 4.3231 - val_loss: 4.2369 - val_MSE: 4.2369\n",
            "Epoch 610/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3156 - MSE: 4.3156 - val_loss: 4.2334 - val_MSE: 4.2334\n",
            "Epoch 611/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3216 - MSE: 4.3216 - val_loss: 4.2663 - val_MSE: 4.2663\n",
            "Epoch 612/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3064 - MSE: 4.3064 - val_loss: 4.2522 - val_MSE: 4.2522\n",
            "Epoch 613/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3053 - MSE: 4.3053 - val_loss: 4.2998 - val_MSE: 4.2998\n",
            "Epoch 614/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3097 - MSE: 4.3097 - val_loss: 4.2751 - val_MSE: 4.2751\n",
            "Epoch 615/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3118 - MSE: 4.3118 - val_loss: 4.2699 - val_MSE: 4.2699\n",
            "Epoch 616/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3083 - MSE: 4.3083 - val_loss: 4.2353 - val_MSE: 4.2353\n",
            "Epoch 617/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3133 - MSE: 4.3133 - val_loss: 4.2436 - val_MSE: 4.2436\n",
            "Epoch 618/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.3131 - MSE: 4.3131 - val_loss: 4.3442 - val_MSE: 4.3442\n",
            "Epoch 619/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3013 - MSE: 4.3013 - val_loss: 4.2447 - val_MSE: 4.2447\n",
            "Epoch 620/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2940 - MSE: 4.2940 - val_loss: 4.2420 - val_MSE: 4.2420\n",
            "Epoch 621/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3042 - MSE: 4.3042 - val_loss: 4.2531 - val_MSE: 4.2531\n",
            "Epoch 622/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3042 - MSE: 4.3042 - val_loss: 4.2409 - val_MSE: 4.2409\n",
            "Epoch 623/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.3048 - MSE: 4.3048 - val_loss: 4.2851 - val_MSE: 4.2851\n",
            "Epoch 624/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2941 - MSE: 4.2941 - val_loss: 4.2355 - val_MSE: 4.2355\n",
            "Epoch 625/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2943 - MSE: 4.2943 - val_loss: 4.2162 - val_MSE: 4.2162\n",
            "Epoch 626/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2917 - MSE: 4.2917 - val_loss: 4.2546 - val_MSE: 4.2546\n",
            "Epoch 627/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2808 - MSE: 4.2808 - val_loss: 4.3968 - val_MSE: 4.3968\n",
            "Epoch 628/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2882 - MSE: 4.2882 - val_loss: 4.3010 - val_MSE: 4.3010\n",
            "Epoch 629/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2948 - MSE: 4.2948 - val_loss: 4.2856 - val_MSE: 4.2856\n",
            "Epoch 630/1000\n",
            "908/908 [==============================] - 1s 1ms/step - loss: 4.2896 - MSE: 4.2896 - val_loss: 4.3041 - val_MSE: 4.3041\n",
            "Epoch 631/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2922 - MSE: 4.2922 - val_loss: 4.2051 - val_MSE: 4.2051\n",
            "Epoch 632/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2818 - MSE: 4.2818 - val_loss: 4.2842 - val_MSE: 4.2842\n",
            "Epoch 633/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2815 - MSE: 4.2815 - val_loss: 4.2334 - val_MSE: 4.2334\n",
            "Epoch 634/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2866 - MSE: 4.2866 - val_loss: 4.3049 - val_MSE: 4.3049\n",
            "Epoch 635/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2764 - MSE: 4.2764 - val_loss: 4.6403 - val_MSE: 4.6403\n",
            "Epoch 636/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2688 - MSE: 4.2688 - val_loss: 4.2581 - val_MSE: 4.2581\n",
            "Epoch 637/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2752 - MSE: 4.2752 - val_loss: 4.2766 - val_MSE: 4.2766\n",
            "Epoch 638/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2728 - MSE: 4.2728 - val_loss: 4.2095 - val_MSE: 4.2095\n",
            "Epoch 639/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2629 - MSE: 4.2629 - val_loss: 4.3798 - val_MSE: 4.3798\n",
            "Epoch 640/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2711 - MSE: 4.2711 - val_loss: 4.2284 - val_MSE: 4.2284\n",
            "Epoch 641/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2766 - MSE: 4.2766 - val_loss: 4.2611 - val_MSE: 4.2611\n",
            "Epoch 642/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2695 - MSE: 4.2695 - val_loss: 4.1974 - val_MSE: 4.1974\n",
            "Epoch 643/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2672 - MSE: 4.2672 - val_loss: 4.2529 - val_MSE: 4.2529\n",
            "Epoch 644/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2539 - MSE: 4.2539 - val_loss: 4.2254 - val_MSE: 4.2254\n",
            "Epoch 645/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2661 - MSE: 4.2661 - val_loss: 4.2790 - val_MSE: 4.2790\n",
            "Epoch 646/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2572 - MSE: 4.2572 - val_loss: 4.2875 - val_MSE: 4.2875\n",
            "Epoch 647/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2663 - MSE: 4.2663 - val_loss: 4.2612 - val_MSE: 4.2612\n",
            "Epoch 648/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2559 - MSE: 4.2559 - val_loss: 4.2307 - val_MSE: 4.2307\n",
            "Epoch 649/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2587 - MSE: 4.2587 - val_loss: 4.1970 - val_MSE: 4.1970\n",
            "Epoch 650/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2647 - MSE: 4.2647 - val_loss: 4.2020 - val_MSE: 4.2020\n",
            "Epoch 651/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2591 - MSE: 4.2591 - val_loss: 4.2045 - val_MSE: 4.2045\n",
            "Epoch 652/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2414 - MSE: 4.2414 - val_loss: 4.2128 - val_MSE: 4.2128\n",
            "Epoch 653/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2469 - MSE: 4.2469 - val_loss: 4.1987 - val_MSE: 4.1987\n",
            "Epoch 654/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2575 - MSE: 4.2575 - val_loss: 4.2182 - val_MSE: 4.2182\n",
            "Epoch 655/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2547 - MSE: 4.2547 - val_loss: 4.2419 - val_MSE: 4.2419\n",
            "Epoch 656/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2442 - MSE: 4.2442 - val_loss: 4.1817 - val_MSE: 4.1817\n",
            "Epoch 657/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2468 - MSE: 4.2468 - val_loss: 4.1824 - val_MSE: 4.1824\n",
            "Epoch 658/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2496 - MSE: 4.2496 - val_loss: 4.1852 - val_MSE: 4.1852\n",
            "Epoch 659/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2471 - MSE: 4.2471 - val_loss: 4.2267 - val_MSE: 4.2267\n",
            "Epoch 660/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2401 - MSE: 4.2401 - val_loss: 4.3708 - val_MSE: 4.3708\n",
            "Epoch 661/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2469 - MSE: 4.2469 - val_loss: 4.2101 - val_MSE: 4.2101\n",
            "Epoch 662/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2513 - MSE: 4.2513 - val_loss: 4.1727 - val_MSE: 4.1727\n",
            "Epoch 663/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2411 - MSE: 4.2411 - val_loss: 4.1995 - val_MSE: 4.1995\n",
            "Epoch 664/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2423 - MSE: 4.2423 - val_loss: 4.2317 - val_MSE: 4.2317\n",
            "Epoch 665/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2405 - MSE: 4.2405 - val_loss: 4.2195 - val_MSE: 4.2195\n",
            "Epoch 666/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2364 - MSE: 4.2364 - val_loss: 4.1800 - val_MSE: 4.1800\n",
            "Epoch 667/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2420 - MSE: 4.2420 - val_loss: 4.1639 - val_MSE: 4.1639\n",
            "Epoch 668/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2360 - MSE: 4.2360 - val_loss: 4.1674 - val_MSE: 4.1674\n",
            "Epoch 669/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2315 - MSE: 4.2315 - val_loss: 4.1993 - val_MSE: 4.1993\n",
            "Epoch 670/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2304 - MSE: 4.2304 - val_loss: 4.1885 - val_MSE: 4.1885\n",
            "Epoch 671/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2263 - MSE: 4.2263 - val_loss: 4.2004 - val_MSE: 4.2004\n",
            "Epoch 672/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2328 - MSE: 4.2328 - val_loss: 4.2506 - val_MSE: 4.2506\n",
            "Epoch 673/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2290 - MSE: 4.2290 - val_loss: 4.2594 - val_MSE: 4.2594\n",
            "Epoch 674/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2289 - MSE: 4.2289 - val_loss: 4.2628 - val_MSE: 4.2628\n",
            "Epoch 675/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2176 - MSE: 4.2176 - val_loss: 4.2439 - val_MSE: 4.2439\n",
            "Epoch 676/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2241 - MSE: 4.2241 - val_loss: 4.2125 - val_MSE: 4.2125\n",
            "Epoch 677/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2206 - MSE: 4.2206 - val_loss: 4.1761 - val_MSE: 4.1761\n",
            "Epoch 678/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2130 - MSE: 4.2130 - val_loss: 4.2377 - val_MSE: 4.2377\n",
            "Epoch 679/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2192 - MSE: 4.2192 - val_loss: 4.1683 - val_MSE: 4.1683\n",
            "Epoch 680/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.2185 - MSE: 4.2185 - val_loss: 4.1812 - val_MSE: 4.1812\n",
            "Epoch 681/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2158 - MSE: 4.2158 - val_loss: 4.2785 - val_MSE: 4.2785\n",
            "Epoch 682/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2177 - MSE: 4.2177 - val_loss: 4.2594 - val_MSE: 4.2594\n",
            "Epoch 683/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2154 - MSE: 4.2154 - val_loss: 4.2117 - val_MSE: 4.2117\n",
            "Epoch 684/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2094 - MSE: 4.2094 - val_loss: 4.1921 - val_MSE: 4.1921\n",
            "Epoch 685/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2081 - MSE: 4.2081 - val_loss: 4.1802 - val_MSE: 4.1802\n",
            "Epoch 686/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2035 - MSE: 4.2035 - val_loss: 4.1407 - val_MSE: 4.1407\n",
            "Epoch 687/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2097 - MSE: 4.2097 - val_loss: 4.2294 - val_MSE: 4.2294\n",
            "Epoch 688/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2056 - MSE: 4.2056 - val_loss: 4.1972 - val_MSE: 4.1972\n",
            "Epoch 689/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2084 - MSE: 4.2084 - val_loss: 4.3015 - val_MSE: 4.3015\n",
            "Epoch 690/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2022 - MSE: 4.2022 - val_loss: 4.2768 - val_MSE: 4.2768\n",
            "Epoch 691/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2055 - MSE: 4.2055 - val_loss: 4.1822 - val_MSE: 4.1822\n",
            "Epoch 692/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2067 - MSE: 4.2067 - val_loss: 4.1870 - val_MSE: 4.1870\n",
            "Epoch 693/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2006 - MSE: 4.2006 - val_loss: 4.2451 - val_MSE: 4.2451\n",
            "Epoch 694/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2002 - MSE: 4.2002 - val_loss: 4.1915 - val_MSE: 4.1915\n",
            "Epoch 695/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.2010 - MSE: 4.2010 - val_loss: 4.1681 - val_MSE: 4.1681\n",
            "Epoch 696/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1965 - MSE: 4.1965 - val_loss: 4.1656 - val_MSE: 4.1656\n",
            "Epoch 697/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1925 - MSE: 4.1925 - val_loss: 4.2013 - val_MSE: 4.2013\n",
            "Epoch 698/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1922 - MSE: 4.1922 - val_loss: 4.1525 - val_MSE: 4.1525\n",
            "Epoch 699/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1988 - MSE: 4.1988 - val_loss: 4.1410 - val_MSE: 4.1410\n",
            "Epoch 700/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1961 - MSE: 4.1961 - val_loss: 4.1629 - val_MSE: 4.1629\n",
            "Epoch 701/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1895 - MSE: 4.1895 - val_loss: 4.2535 - val_MSE: 4.2535\n",
            "Epoch 702/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1905 - MSE: 4.1905 - val_loss: 4.1578 - val_MSE: 4.1578\n",
            "Epoch 703/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1865 - MSE: 4.1865 - val_loss: 4.2028 - val_MSE: 4.2028\n",
            "Epoch 704/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1950 - MSE: 4.1950 - val_loss: 4.2147 - val_MSE: 4.2147\n",
            "Epoch 705/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1897 - MSE: 4.1897 - val_loss: 4.1474 - val_MSE: 4.1474\n",
            "Epoch 706/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1853 - MSE: 4.1853 - val_loss: 4.1312 - val_MSE: 4.1312\n",
            "Epoch 707/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1938 - MSE: 4.1938 - val_loss: 4.2049 - val_MSE: 4.2049\n",
            "Epoch 708/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1867 - MSE: 4.1867 - val_loss: 4.1170 - val_MSE: 4.1170\n",
            "Epoch 709/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1800 - MSE: 4.1800 - val_loss: 4.1553 - val_MSE: 4.1553\n",
            "Epoch 710/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1802 - MSE: 4.1802 - val_loss: 4.1818 - val_MSE: 4.1818\n",
            "Epoch 711/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1799 - MSE: 4.1799 - val_loss: 4.2229 - val_MSE: 4.2229\n",
            "Epoch 712/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1765 - MSE: 4.1765 - val_loss: 4.1648 - val_MSE: 4.1648\n",
            "Epoch 713/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1836 - MSE: 4.1836 - val_loss: 4.2405 - val_MSE: 4.2405\n",
            "Epoch 714/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1727 - MSE: 4.1727 - val_loss: 4.1635 - val_MSE: 4.1635\n",
            "Epoch 715/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1786 - MSE: 4.1786 - val_loss: 4.1486 - val_MSE: 4.1486\n",
            "Epoch 716/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1714 - MSE: 4.1714 - val_loss: 4.1977 - val_MSE: 4.1977\n",
            "Epoch 717/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1727 - MSE: 4.1727 - val_loss: 4.2621 - val_MSE: 4.2621\n",
            "Epoch 718/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1770 - MSE: 4.1770 - val_loss: 4.1655 - val_MSE: 4.1655\n",
            "Epoch 719/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1731 - MSE: 4.1731 - val_loss: 4.1356 - val_MSE: 4.1356\n",
            "Epoch 720/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1740 - MSE: 4.1740 - val_loss: 4.1676 - val_MSE: 4.1676\n",
            "Epoch 721/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1738 - MSE: 4.1738 - val_loss: 4.2514 - val_MSE: 4.2514\n",
            "Epoch 722/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1665 - MSE: 4.1665 - val_loss: 4.1539 - val_MSE: 4.1539\n",
            "Epoch 723/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1751 - MSE: 4.1751 - val_loss: 4.1802 - val_MSE: 4.1802\n",
            "Epoch 724/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1767 - MSE: 4.1767 - val_loss: 4.2265 - val_MSE: 4.2265\n",
            "Epoch 725/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1603 - MSE: 4.1603 - val_loss: 4.1865 - val_MSE: 4.1865\n",
            "Epoch 726/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1598 - MSE: 4.1598 - val_loss: 4.1312 - val_MSE: 4.1312\n",
            "Epoch 727/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1638 - MSE: 4.1638 - val_loss: 4.1766 - val_MSE: 4.1766\n",
            "Epoch 728/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1625 - MSE: 4.1625 - val_loss: 4.1624 - val_MSE: 4.1624\n",
            "Epoch 729/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1576 - MSE: 4.1576 - val_loss: 4.1924 - val_MSE: 4.1924\n",
            "Epoch 730/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1606 - MSE: 4.1606 - val_loss: 4.1573 - val_MSE: 4.1573\n",
            "Epoch 731/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1595 - MSE: 4.1595 - val_loss: 4.1529 - val_MSE: 4.1529\n",
            "Epoch 732/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1525 - MSE: 4.1525 - val_loss: 4.1084 - val_MSE: 4.1084\n",
            "Epoch 733/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1628 - MSE: 4.1628 - val_loss: 4.2239 - val_MSE: 4.2239\n",
            "Epoch 734/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1683 - MSE: 4.1683 - val_loss: 4.1377 - val_MSE: 4.1377\n",
            "Epoch 735/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1542 - MSE: 4.1542 - val_loss: 4.1465 - val_MSE: 4.1465\n",
            "Epoch 736/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1570 - MSE: 4.1570 - val_loss: 4.2101 - val_MSE: 4.2101\n",
            "Epoch 737/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1580 - MSE: 4.1580 - val_loss: 4.1668 - val_MSE: 4.1668\n",
            "Epoch 738/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1539 - MSE: 4.1539 - val_loss: 4.1724 - val_MSE: 4.1724\n",
            "Epoch 739/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1533 - MSE: 4.1533 - val_loss: 4.1521 - val_MSE: 4.1521\n",
            "Epoch 740/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1515 - MSE: 4.1515 - val_loss: 4.1191 - val_MSE: 4.1191\n",
            "Epoch 741/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1403 - MSE: 4.1403 - val_loss: 4.1976 - val_MSE: 4.1976\n",
            "Epoch 742/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1583 - MSE: 4.1583 - val_loss: 4.1394 - val_MSE: 4.1394\n",
            "Epoch 743/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1598 - MSE: 4.1598 - val_loss: 4.1128 - val_MSE: 4.1128\n",
            "Epoch 744/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1552 - MSE: 4.1552 - val_loss: 4.1712 - val_MSE: 4.1712\n",
            "Epoch 745/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1422 - MSE: 4.1422 - val_loss: 4.1683 - val_MSE: 4.1683\n",
            "Epoch 746/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1525 - MSE: 4.1525 - val_loss: 4.1395 - val_MSE: 4.1395\n",
            "Epoch 747/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1450 - MSE: 4.1450 - val_loss: 4.1338 - val_MSE: 4.1338\n",
            "Epoch 748/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1417 - MSE: 4.1417 - val_loss: 4.1853 - val_MSE: 4.1853\n",
            "Epoch 749/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1389 - MSE: 4.1389 - val_loss: 4.1107 - val_MSE: 4.1107\n",
            "Epoch 750/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1399 - MSE: 4.1399 - val_loss: 4.2565 - val_MSE: 4.2565\n",
            "Epoch 751/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1291 - MSE: 4.1291 - val_loss: 4.1622 - val_MSE: 4.1622\n",
            "Epoch 752/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1480 - MSE: 4.1480 - val_loss: 4.1085 - val_MSE: 4.1085\n",
            "Epoch 753/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1454 - MSE: 4.1454 - val_loss: 4.1597 - val_MSE: 4.1597\n",
            "Epoch 754/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1353 - MSE: 4.1353 - val_loss: 4.1223 - val_MSE: 4.1223\n",
            "Epoch 755/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1401 - MSE: 4.1401 - val_loss: 4.1253 - val_MSE: 4.1253\n",
            "Epoch 756/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1394 - MSE: 4.1394 - val_loss: 4.1083 - val_MSE: 4.1083\n",
            "Epoch 757/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1373 - MSE: 4.1373 - val_loss: 4.1223 - val_MSE: 4.1223\n",
            "Epoch 758/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1312 - MSE: 4.1312 - val_loss: 4.1636 - val_MSE: 4.1636\n",
            "Epoch 759/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1264 - MSE: 4.1264 - val_loss: 4.1569 - val_MSE: 4.1569\n",
            "Epoch 760/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1432 - MSE: 4.1432 - val_loss: 4.1102 - val_MSE: 4.1102\n",
            "Epoch 761/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1333 - MSE: 4.1333 - val_loss: 4.0816 - val_MSE: 4.0816\n",
            "Epoch 762/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1200 - MSE: 4.1200 - val_loss: 4.1202 - val_MSE: 4.1202\n",
            "Epoch 763/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1340 - MSE: 4.1340 - val_loss: 4.1234 - val_MSE: 4.1234\n",
            "Epoch 764/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1293 - MSE: 4.1293 - val_loss: 4.1702 - val_MSE: 4.1702\n",
            "Epoch 765/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1268 - MSE: 4.1268 - val_loss: 4.1485 - val_MSE: 4.1485\n",
            "Epoch 766/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1276 - MSE: 4.1276 - val_loss: 4.1612 - val_MSE: 4.1612\n",
            "Epoch 767/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1228 - MSE: 4.1228 - val_loss: 4.0985 - val_MSE: 4.0985\n",
            "Epoch 768/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1322 - MSE: 4.1322 - val_loss: 4.1076 - val_MSE: 4.1076\n",
            "Epoch 769/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1204 - MSE: 4.1204 - val_loss: 4.1519 - val_MSE: 4.1519\n",
            "Epoch 770/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1284 - MSE: 4.1284 - val_loss: 4.1516 - val_MSE: 4.1516\n",
            "Epoch 771/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1125 - MSE: 4.1125 - val_loss: 4.1181 - val_MSE: 4.1181\n",
            "Epoch 772/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1294 - MSE: 4.1294 - val_loss: 4.1431 - val_MSE: 4.1431\n",
            "Epoch 773/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1152 - MSE: 4.1152 - val_loss: 4.1183 - val_MSE: 4.1183\n",
            "Epoch 774/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1202 - MSE: 4.1202 - val_loss: 4.2109 - val_MSE: 4.2109\n",
            "Epoch 775/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1286 - MSE: 4.1286 - val_loss: 4.0801 - val_MSE: 4.0801\n",
            "Epoch 776/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1099 - MSE: 4.1099 - val_loss: 4.1150 - val_MSE: 4.1150\n",
            "Epoch 777/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1257 - MSE: 4.1257 - val_loss: 4.1989 - val_MSE: 4.1989\n",
            "Epoch 778/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1187 - MSE: 4.1187 - val_loss: 4.1110 - val_MSE: 4.1110\n",
            "Epoch 779/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1184 - MSE: 4.1184 - val_loss: 4.1263 - val_MSE: 4.1263\n",
            "Epoch 780/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1183 - MSE: 4.1183 - val_loss: 4.1249 - val_MSE: 4.1249\n",
            "Epoch 781/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1099 - MSE: 4.1099 - val_loss: 4.2439 - val_MSE: 4.2439\n",
            "Epoch 782/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1089 - MSE: 4.1089 - val_loss: 4.1079 - val_MSE: 4.1079\n",
            "Epoch 783/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1057 - MSE: 4.1057 - val_loss: 4.1325 - val_MSE: 4.1325\n",
            "Epoch 784/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1085 - MSE: 4.1085 - val_loss: 4.1688 - val_MSE: 4.1688\n",
            "Epoch 785/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1078 - MSE: 4.1078 - val_loss: 4.1304 - val_MSE: 4.1304\n",
            "Epoch 786/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1199 - MSE: 4.1199 - val_loss: 4.0669 - val_MSE: 4.0669\n",
            "Epoch 787/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1063 - MSE: 4.1063 - val_loss: 4.1693 - val_MSE: 4.1693\n",
            "Epoch 788/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1022 - MSE: 4.1022 - val_loss: 4.1421 - val_MSE: 4.1421\n",
            "Epoch 789/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1107 - MSE: 4.1107 - val_loss: 4.1175 - val_MSE: 4.1175\n",
            "Epoch 790/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1116 - MSE: 4.1116 - val_loss: 4.1132 - val_MSE: 4.1132\n",
            "Epoch 791/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1141 - MSE: 4.1141 - val_loss: 4.0818 - val_MSE: 4.0818\n",
            "Epoch 792/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1034 - MSE: 4.1034 - val_loss: 4.0900 - val_MSE: 4.0900\n",
            "Epoch 793/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0992 - MSE: 4.0992 - val_loss: 4.0867 - val_MSE: 4.0867\n",
            "Epoch 794/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0963 - MSE: 4.0963 - val_loss: 4.0725 - val_MSE: 4.0725\n",
            "Epoch 795/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1031 - MSE: 4.1031 - val_loss: 4.1492 - val_MSE: 4.1492\n",
            "Epoch 796/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1050 - MSE: 4.1050 - val_loss: 4.0536 - val_MSE: 4.0536\n",
            "Epoch 797/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1062 - MSE: 4.1062 - val_loss: 4.1147 - val_MSE: 4.1147\n",
            "Epoch 798/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0943 - MSE: 4.0943 - val_loss: 4.1018 - val_MSE: 4.1018\n",
            "Epoch 799/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.1032 - MSE: 4.1032 - val_loss: 4.1176 - val_MSE: 4.1176\n",
            "Epoch 800/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0956 - MSE: 4.0956 - val_loss: 4.0884 - val_MSE: 4.0884\n",
            "Epoch 801/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0898 - MSE: 4.0898 - val_loss: 4.1171 - val_MSE: 4.1171\n",
            "Epoch 802/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0989 - MSE: 4.0989 - val_loss: 4.0889 - val_MSE: 4.0889\n",
            "Epoch 803/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0941 - MSE: 4.0941 - val_loss: 4.1267 - val_MSE: 4.1267\n",
            "Epoch 804/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0811 - MSE: 4.0811 - val_loss: 4.1821 - val_MSE: 4.1821\n",
            "Epoch 805/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0943 - MSE: 4.0943 - val_loss: 4.0848 - val_MSE: 4.0848\n",
            "Epoch 806/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0926 - MSE: 4.0926 - val_loss: 4.0656 - val_MSE: 4.0656\n",
            "Epoch 807/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.1030 - MSE: 4.1030 - val_loss: 4.0813 - val_MSE: 4.0813\n",
            "Epoch 808/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0897 - MSE: 4.0897 - val_loss: 4.0821 - val_MSE: 4.0821\n",
            "Epoch 809/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0942 - MSE: 4.0942 - val_loss: 4.0796 - val_MSE: 4.0796\n",
            "Epoch 810/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0814 - MSE: 4.0814 - val_loss: 4.0783 - val_MSE: 4.0783\n",
            "Epoch 811/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0904 - MSE: 4.0904 - val_loss: 4.0718 - val_MSE: 4.0718\n",
            "Epoch 812/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0860 - MSE: 4.0860 - val_loss: 4.0608 - val_MSE: 4.0608\n",
            "Epoch 813/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0861 - MSE: 4.0861 - val_loss: 4.1143 - val_MSE: 4.1143\n",
            "Epoch 814/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0742 - MSE: 4.0742 - val_loss: 4.0943 - val_MSE: 4.0943\n",
            "Epoch 815/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0934 - MSE: 4.0934 - val_loss: 4.0442 - val_MSE: 4.0442\n",
            "Epoch 816/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0869 - MSE: 4.0869 - val_loss: 4.0665 - val_MSE: 4.0665\n",
            "Epoch 817/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0845 - MSE: 4.0845 - val_loss: 4.0752 - val_MSE: 4.0752\n",
            "Epoch 818/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0841 - MSE: 4.0841 - val_loss: 4.0401 - val_MSE: 4.0401\n",
            "Epoch 819/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0863 - MSE: 4.0863 - val_loss: 4.2699 - val_MSE: 4.2699\n",
            "Epoch 820/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0821 - MSE: 4.0821 - val_loss: 4.0451 - val_MSE: 4.0451\n",
            "Epoch 821/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0800 - MSE: 4.0800 - val_loss: 4.0875 - val_MSE: 4.0875\n",
            "Epoch 822/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0795 - MSE: 4.0795 - val_loss: 4.0502 - val_MSE: 4.0502\n",
            "Epoch 823/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0818 - MSE: 4.0818 - val_loss: 4.1851 - val_MSE: 4.1851\n",
            "Epoch 824/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0821 - MSE: 4.0821 - val_loss: 4.0782 - val_MSE: 4.0782\n",
            "Epoch 825/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0856 - MSE: 4.0856 - val_loss: 4.0526 - val_MSE: 4.0526\n",
            "Epoch 826/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0875 - MSE: 4.0875 - val_loss: 4.0589 - val_MSE: 4.0589\n",
            "Epoch 827/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0649 - MSE: 4.0649 - val_loss: 4.0617 - val_MSE: 4.0617\n",
            "Epoch 828/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0750 - MSE: 4.0750 - val_loss: 4.0705 - val_MSE: 4.0705\n",
            "Epoch 829/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0792 - MSE: 4.0792 - val_loss: 4.0816 - val_MSE: 4.0816\n",
            "Epoch 830/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0829 - MSE: 4.0829 - val_loss: 4.0522 - val_MSE: 4.0522\n",
            "Epoch 831/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0743 - MSE: 4.0743 - val_loss: 4.0953 - val_MSE: 4.0953\n",
            "Epoch 832/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0814 - MSE: 4.0814 - val_loss: 4.0662 - val_MSE: 4.0662\n",
            "Epoch 833/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0663 - MSE: 4.0663 - val_loss: 4.0875 - val_MSE: 4.0875\n",
            "Epoch 834/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0661 - MSE: 4.0661 - val_loss: 4.0839 - val_MSE: 4.0839\n",
            "Epoch 835/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0693 - MSE: 4.0693 - val_loss: 4.0984 - val_MSE: 4.0984\n",
            "Epoch 836/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0754 - MSE: 4.0754 - val_loss: 4.0758 - val_MSE: 4.0758\n",
            "Epoch 837/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0772 - MSE: 4.0772 - val_loss: 4.0669 - val_MSE: 4.0669\n",
            "Epoch 838/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0629 - MSE: 4.0629 - val_loss: 4.0742 - val_MSE: 4.0742\n",
            "Epoch 839/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0642 - MSE: 4.0642 - val_loss: 4.0767 - val_MSE: 4.0767\n",
            "Epoch 840/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0719 - MSE: 4.0719 - val_loss: 4.0572 - val_MSE: 4.0572\n",
            "Epoch 841/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0538 - MSE: 4.0538 - val_loss: 4.0871 - val_MSE: 4.0871\n",
            "Epoch 842/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0625 - MSE: 4.0625 - val_loss: 4.0734 - val_MSE: 4.0734\n",
            "Epoch 843/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0630 - MSE: 4.0630 - val_loss: 4.1210 - val_MSE: 4.1210\n",
            "Epoch 844/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0661 - MSE: 4.0661 - val_loss: 4.0446 - val_MSE: 4.0446\n",
            "Epoch 845/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0570 - MSE: 4.0570 - val_loss: 4.1156 - val_MSE: 4.1156\n",
            "Epoch 846/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0635 - MSE: 4.0635 - val_loss: 4.0971 - val_MSE: 4.0971\n",
            "Epoch 847/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0619 - MSE: 4.0619 - val_loss: 4.0537 - val_MSE: 4.0537\n",
            "Epoch 848/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0538 - MSE: 4.0538 - val_loss: 4.1558 - val_MSE: 4.1558\n",
            "Epoch 849/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0676 - MSE: 4.0676 - val_loss: 4.0335 - val_MSE: 4.0335\n",
            "Epoch 850/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0549 - MSE: 4.0549 - val_loss: 4.0522 - val_MSE: 4.0522\n",
            "Epoch 851/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0516 - MSE: 4.0516 - val_loss: 4.0426 - val_MSE: 4.0426\n",
            "Epoch 852/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0536 - MSE: 4.0536 - val_loss: 4.0192 - val_MSE: 4.0192\n",
            "Epoch 853/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0445 - MSE: 4.0445 - val_loss: 4.0552 - val_MSE: 4.0552\n",
            "Epoch 854/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0561 - MSE: 4.0561 - val_loss: 4.0542 - val_MSE: 4.0542\n",
            "Epoch 855/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0583 - MSE: 4.0583 - val_loss: 4.0999 - val_MSE: 4.0999\n",
            "Epoch 856/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0409 - MSE: 4.0409 - val_loss: 4.1319 - val_MSE: 4.1319\n",
            "Epoch 857/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0547 - MSE: 4.0547 - val_loss: 4.0941 - val_MSE: 4.0941\n",
            "Epoch 858/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0552 - MSE: 4.0552 - val_loss: 4.0755 - val_MSE: 4.0755\n",
            "Epoch 859/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0438 - MSE: 4.0438 - val_loss: 4.0710 - val_MSE: 4.0710\n",
            "Epoch 860/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0547 - MSE: 4.0547 - val_loss: 4.0366 - val_MSE: 4.0366\n",
            "Epoch 861/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0430 - MSE: 4.0430 - val_loss: 4.1004 - val_MSE: 4.1004\n",
            "Epoch 862/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0495 - MSE: 4.0495 - val_loss: 4.1045 - val_MSE: 4.1045\n",
            "Epoch 863/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0414 - MSE: 4.0414 - val_loss: 4.0895 - val_MSE: 4.0895\n",
            "Epoch 864/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0501 - MSE: 4.0501 - val_loss: 4.0813 - val_MSE: 4.0813\n",
            "Epoch 865/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0423 - MSE: 4.0423 - val_loss: 4.0258 - val_MSE: 4.0258\n",
            "Epoch 866/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0476 - MSE: 4.0476 - val_loss: 4.0911 - val_MSE: 4.0911\n",
            "Epoch 867/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0408 - MSE: 4.0408 - val_loss: 4.0487 - val_MSE: 4.0487\n",
            "Epoch 868/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0410 - MSE: 4.0410 - val_loss: 4.0700 - val_MSE: 4.0700\n",
            "Epoch 869/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0310 - MSE: 4.0310 - val_loss: 4.0609 - val_MSE: 4.0609\n",
            "Epoch 870/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0336 - MSE: 4.0336 - val_loss: 4.0403 - val_MSE: 4.0403\n",
            "Epoch 871/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0353 - MSE: 4.0353 - val_loss: 4.1000 - val_MSE: 4.1000\n",
            "Epoch 872/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0398 - MSE: 4.0398 - val_loss: 4.0382 - val_MSE: 4.0382\n",
            "Epoch 873/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0389 - MSE: 4.0389 - val_loss: 4.0806 - val_MSE: 4.0806\n",
            "Epoch 874/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0337 - MSE: 4.0337 - val_loss: 4.0703 - val_MSE: 4.0703\n",
            "Epoch 875/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0349 - MSE: 4.0349 - val_loss: 4.0469 - val_MSE: 4.0469\n",
            "Epoch 876/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0339 - MSE: 4.0339 - val_loss: 4.0521 - val_MSE: 4.0521\n",
            "Epoch 877/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0400 - MSE: 4.0400 - val_loss: 4.0336 - val_MSE: 4.0336\n",
            "Epoch 878/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0244 - MSE: 4.0244 - val_loss: 4.0936 - val_MSE: 4.0936\n",
            "Epoch 879/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0309 - MSE: 4.0309 - val_loss: 4.0597 - val_MSE: 4.0597\n",
            "Epoch 880/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0331 - MSE: 4.0331 - val_loss: 4.0572 - val_MSE: 4.0572\n",
            "Epoch 881/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0251 - MSE: 4.0251 - val_loss: 4.0685 - val_MSE: 4.0685\n",
            "Epoch 882/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0382 - MSE: 4.0382 - val_loss: 4.0281 - val_MSE: 4.0281\n",
            "Epoch 883/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0234 - MSE: 4.0234 - val_loss: 4.0612 - val_MSE: 4.0612\n",
            "Epoch 884/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0334 - MSE: 4.0334 - val_loss: 4.0594 - val_MSE: 4.0594\n",
            "Epoch 885/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0240 - MSE: 4.0240 - val_loss: 4.0473 - val_MSE: 4.0473\n",
            "Epoch 886/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0190 - MSE: 4.0190 - val_loss: 4.0732 - val_MSE: 4.0732\n",
            "Epoch 887/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0330 - MSE: 4.0330 - val_loss: 4.0418 - val_MSE: 4.0418\n",
            "Epoch 888/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0272 - MSE: 4.0272 - val_loss: 4.0016 - val_MSE: 4.0016\n",
            "Epoch 889/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0268 - MSE: 4.0268 - val_loss: 4.0397 - val_MSE: 4.0397\n",
            "Epoch 890/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0286 - MSE: 4.0286 - val_loss: 4.0654 - val_MSE: 4.0654\n",
            "Epoch 891/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0254 - MSE: 4.0254 - val_loss: 4.0073 - val_MSE: 4.0073\n",
            "Epoch 892/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0211 - MSE: 4.0211 - val_loss: 4.0312 - val_MSE: 4.0312\n",
            "Epoch 893/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0217 - MSE: 4.0217 - val_loss: 4.0043 - val_MSE: 4.0043\n",
            "Epoch 894/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0218 - MSE: 4.0218 - val_loss: 4.0563 - val_MSE: 4.0563\n",
            "Epoch 895/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0139 - MSE: 4.0139 - val_loss: 4.0359 - val_MSE: 4.0359\n",
            "Epoch 896/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0265 - MSE: 4.0265 - val_loss: 3.9982 - val_MSE: 3.9982\n",
            "Epoch 897/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0201 - MSE: 4.0201 - val_loss: 3.9889 - val_MSE: 3.9889\n",
            "Epoch 898/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0083 - MSE: 4.0083 - val_loss: 4.0320 - val_MSE: 4.0320\n",
            "Epoch 899/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0112 - MSE: 4.0112 - val_loss: 4.0365 - val_MSE: 4.0365\n",
            "Epoch 900/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0133 - MSE: 4.0133 - val_loss: 4.0255 - val_MSE: 4.0255\n",
            "Epoch 901/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0073 - MSE: 4.0073 - val_loss: 3.9902 - val_MSE: 3.9902\n",
            "Epoch 902/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9973 - MSE: 3.9973 - val_loss: 4.0571 - val_MSE: 4.0571\n",
            "Epoch 903/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0103 - MSE: 4.0103 - val_loss: 4.0405 - val_MSE: 4.0405\n",
            "Epoch 904/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0088 - MSE: 4.0088 - val_loss: 4.2514 - val_MSE: 4.2514\n",
            "Epoch 905/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0061 - MSE: 4.0061 - val_loss: 4.0294 - val_MSE: 4.0294\n",
            "Epoch 906/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0110 - MSE: 4.0110 - val_loss: 4.0324 - val_MSE: 4.0324\n",
            "Epoch 907/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9989 - MSE: 3.9989 - val_loss: 3.9821 - val_MSE: 3.9821\n",
            "Epoch 908/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0092 - MSE: 4.0092 - val_loss: 4.0887 - val_MSE: 4.0887\n",
            "Epoch 909/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0024 - MSE: 4.0024 - val_loss: 4.0056 - val_MSE: 4.0056\n",
            "Epoch 910/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9944 - MSE: 3.9944 - val_loss: 4.0280 - val_MSE: 4.0280\n",
            "Epoch 911/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0091 - MSE: 4.0091 - val_loss: 3.9929 - val_MSE: 3.9929\n",
            "Epoch 912/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0034 - MSE: 4.0034 - val_loss: 4.0036 - val_MSE: 4.0036\n",
            "Epoch 913/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9841 - MSE: 3.9841 - val_loss: 3.9906 - val_MSE: 3.9906\n",
            "Epoch 914/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0023 - MSE: 4.0023 - val_loss: 3.9992 - val_MSE: 3.9992\n",
            "Epoch 915/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9980 - MSE: 3.9980 - val_loss: 3.9945 - val_MSE: 3.9945\n",
            "Epoch 916/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0041 - MSE: 4.0041 - val_loss: 3.9595 - val_MSE: 3.9595\n",
            "Epoch 917/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 4.0016 - MSE: 4.0016 - val_loss: 4.0077 - val_MSE: 4.0077\n",
            "Epoch 918/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 4.0073 - MSE: 4.0073 - val_loss: 4.0621 - val_MSE: 4.0621\n",
            "Epoch 919/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9907 - MSE: 3.9907 - val_loss: 4.0277 - val_MSE: 4.0277\n",
            "Epoch 920/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9891 - MSE: 3.9891 - val_loss: 4.0403 - val_MSE: 4.0403\n",
            "Epoch 921/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9988 - MSE: 3.9988 - val_loss: 4.0028 - val_MSE: 4.0028\n",
            "Epoch 922/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9948 - MSE: 3.9948 - val_loss: 4.0193 - val_MSE: 4.0193\n",
            "Epoch 923/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9858 - MSE: 3.9858 - val_loss: 4.0613 - val_MSE: 4.0613\n",
            "Epoch 924/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9852 - MSE: 3.9852 - val_loss: 3.9602 - val_MSE: 3.9602\n",
            "Epoch 925/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9900 - MSE: 3.9900 - val_loss: 4.0247 - val_MSE: 4.0247\n",
            "Epoch 926/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9848 - MSE: 3.9848 - val_loss: 4.0999 - val_MSE: 4.0999\n",
            "Epoch 927/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9974 - MSE: 3.9974 - val_loss: 4.1021 - val_MSE: 4.1021\n",
            "Epoch 928/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9801 - MSE: 3.9801 - val_loss: 3.9775 - val_MSE: 3.9775\n",
            "Epoch 929/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9975 - MSE: 3.9975 - val_loss: 3.9896 - val_MSE: 3.9896\n",
            "Epoch 930/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9797 - MSE: 3.9797 - val_loss: 4.1026 - val_MSE: 4.1026\n",
            "Epoch 931/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9811 - MSE: 3.9811 - val_loss: 3.9989 - val_MSE: 3.9989\n",
            "Epoch 932/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9850 - MSE: 3.9850 - val_loss: 4.0005 - val_MSE: 4.0005\n",
            "Epoch 933/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9803 - MSE: 3.9803 - val_loss: 4.0833 - val_MSE: 4.0833\n",
            "Epoch 934/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9932 - MSE: 3.9932 - val_loss: 4.0479 - val_MSE: 4.0479\n",
            "Epoch 935/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9778 - MSE: 3.9778 - val_loss: 3.9581 - val_MSE: 3.9581\n",
            "Epoch 936/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9828 - MSE: 3.9828 - val_loss: 3.9695 - val_MSE: 3.9695\n",
            "Epoch 937/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9842 - MSE: 3.9842 - val_loss: 3.9915 - val_MSE: 3.9915\n",
            "Epoch 938/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9861 - MSE: 3.9861 - val_loss: 3.9830 - val_MSE: 3.9830\n",
            "Epoch 939/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9825 - MSE: 3.9825 - val_loss: 4.0125 - val_MSE: 4.0125\n",
            "Epoch 940/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9780 - MSE: 3.9780 - val_loss: 3.9678 - val_MSE: 3.9678\n",
            "Epoch 941/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9653 - MSE: 3.9653 - val_loss: 4.0334 - val_MSE: 4.0334\n",
            "Epoch 942/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9842 - MSE: 3.9842 - val_loss: 4.0411 - val_MSE: 4.0411\n",
            "Epoch 943/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9842 - MSE: 3.9842 - val_loss: 3.9676 - val_MSE: 3.9676\n",
            "Epoch 944/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9711 - MSE: 3.9711 - val_loss: 3.9757 - val_MSE: 3.9757\n",
            "Epoch 945/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9767 - MSE: 3.9767 - val_loss: 4.0930 - val_MSE: 4.0930\n",
            "Epoch 946/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9698 - MSE: 3.9698 - val_loss: 3.9860 - val_MSE: 3.9860\n",
            "Epoch 947/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9685 - MSE: 3.9685 - val_loss: 4.0110 - val_MSE: 4.0110\n",
            "Epoch 948/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9712 - MSE: 3.9712 - val_loss: 3.9564 - val_MSE: 3.9564\n",
            "Epoch 949/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9702 - MSE: 3.9702 - val_loss: 3.9703 - val_MSE: 3.9703\n",
            "Epoch 950/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9683 - MSE: 3.9683 - val_loss: 4.0316 - val_MSE: 4.0316\n",
            "Epoch 951/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9668 - MSE: 3.9668 - val_loss: 3.9556 - val_MSE: 3.9556\n",
            "Epoch 952/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9810 - MSE: 3.9810 - val_loss: 4.0089 - val_MSE: 4.0089\n",
            "Epoch 953/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9627 - MSE: 3.9627 - val_loss: 3.9687 - val_MSE: 3.9687\n",
            "Epoch 954/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9810 - MSE: 3.9810 - val_loss: 4.0052 - val_MSE: 4.0052\n",
            "Epoch 955/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9673 - MSE: 3.9673 - val_loss: 4.0373 - val_MSE: 4.0373\n",
            "Epoch 956/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9652 - MSE: 3.9652 - val_loss: 4.0015 - val_MSE: 4.0015\n",
            "Epoch 957/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9584 - MSE: 3.9584 - val_loss: 3.9776 - val_MSE: 3.9776\n",
            "Epoch 958/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9662 - MSE: 3.9662 - val_loss: 3.9998 - val_MSE: 3.9998\n",
            "Epoch 959/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9681 - MSE: 3.9681 - val_loss: 3.9491 - val_MSE: 3.9491\n",
            "Epoch 960/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9686 - MSE: 3.9686 - val_loss: 3.9418 - val_MSE: 3.9418\n",
            "Epoch 961/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9630 - MSE: 3.9630 - val_loss: 3.9992 - val_MSE: 3.9992\n",
            "Epoch 962/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9619 - MSE: 3.9619 - val_loss: 3.9752 - val_MSE: 3.9752\n",
            "Epoch 963/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9577 - MSE: 3.9577 - val_loss: 3.9479 - val_MSE: 3.9479\n",
            "Epoch 964/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9611 - MSE: 3.9611 - val_loss: 3.9911 - val_MSE: 3.9911\n",
            "Epoch 965/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9461 - MSE: 3.9461 - val_loss: 3.9959 - val_MSE: 3.9959\n",
            "Epoch 966/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9610 - MSE: 3.9610 - val_loss: 3.9574 - val_MSE: 3.9574\n",
            "Epoch 967/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9626 - MSE: 3.9626 - val_loss: 4.0379 - val_MSE: 4.0379\n",
            "Epoch 968/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9610 - MSE: 3.9610 - val_loss: 3.9723 - val_MSE: 3.9723\n",
            "Epoch 969/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9626 - MSE: 3.9626 - val_loss: 4.0803 - val_MSE: 4.0803\n",
            "Epoch 970/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9604 - MSE: 3.9604 - val_loss: 3.9394 - val_MSE: 3.9394\n",
            "Epoch 971/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9529 - MSE: 3.9529 - val_loss: 3.9891 - val_MSE: 3.9891\n",
            "Epoch 972/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9532 - MSE: 3.9532 - val_loss: 4.0872 - val_MSE: 4.0872\n",
            "Epoch 973/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9562 - MSE: 3.9562 - val_loss: 3.9661 - val_MSE: 3.9661\n",
            "Epoch 974/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9493 - MSE: 3.9493 - val_loss: 3.9600 - val_MSE: 3.9600\n",
            "Epoch 975/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9509 - MSE: 3.9509 - val_loss: 3.9733 - val_MSE: 3.9733\n",
            "Epoch 976/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9483 - MSE: 3.9483 - val_loss: 3.9480 - val_MSE: 3.9480\n",
            "Epoch 977/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9510 - MSE: 3.9510 - val_loss: 3.9151 - val_MSE: 3.9151\n",
            "Epoch 978/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9472 - MSE: 3.9472 - val_loss: 3.9778 - val_MSE: 3.9778\n",
            "Epoch 979/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9448 - MSE: 3.9448 - val_loss: 3.9814 - val_MSE: 3.9814\n",
            "Epoch 980/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9546 - MSE: 3.9546 - val_loss: 3.9489 - val_MSE: 3.9489\n",
            "Epoch 981/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9455 - MSE: 3.9455 - val_loss: 3.9902 - val_MSE: 3.9902\n",
            "Epoch 982/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9326 - MSE: 3.9326 - val_loss: 3.9958 - val_MSE: 3.9958\n",
            "Epoch 983/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9413 - MSE: 3.9413 - val_loss: 3.9430 - val_MSE: 3.9430\n",
            "Epoch 984/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9522 - MSE: 3.9522 - val_loss: 3.9826 - val_MSE: 3.9826\n",
            "Epoch 985/1000\n",
            "908/908 [==============================] - 3s 3ms/step - loss: 3.9450 - MSE: 3.9450 - val_loss: 3.9619 - val_MSE: 3.9619\n",
            "Epoch 986/1000\n",
            "908/908 [==============================] - 2s 3ms/step - loss: 3.9380 - MSE: 3.9380 - val_loss: 4.0526 - val_MSE: 4.0526\n",
            "Epoch 987/1000\n",
            "908/908 [==============================] - 4s 5ms/step - loss: 3.9443 - MSE: 3.9443 - val_loss: 3.9748 - val_MSE: 3.9748\n",
            "Epoch 988/1000\n",
            "908/908 [==============================] - 3s 3ms/step - loss: 3.9391 - MSE: 3.9391 - val_loss: 3.9383 - val_MSE: 3.9383\n",
            "Epoch 989/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9529 - MSE: 3.9529 - val_loss: 3.9534 - val_MSE: 3.9534\n",
            "Epoch 990/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9406 - MSE: 3.9406 - val_loss: 3.9534 - val_MSE: 3.9534\n",
            "Epoch 991/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9383 - MSE: 3.9383 - val_loss: 3.9588 - val_MSE: 3.9588\n",
            "Epoch 992/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9437 - MSE: 3.9437 - val_loss: 4.0071 - val_MSE: 4.0071\n",
            "Epoch 993/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9371 - MSE: 3.9371 - val_loss: 4.0014 - val_MSE: 4.0014\n",
            "Epoch 994/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9345 - MSE: 3.9345 - val_loss: 3.9217 - val_MSE: 3.9217\n",
            "Epoch 995/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9384 - MSE: 3.9384 - val_loss: 3.9669 - val_MSE: 3.9669\n",
            "Epoch 996/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9308 - MSE: 3.9308 - val_loss: 3.9264 - val_MSE: 3.9264\n",
            "Epoch 997/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9293 - MSE: 3.9293 - val_loss: 3.9472 - val_MSE: 3.9472\n",
            "Epoch 998/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9390 - MSE: 3.9390 - val_loss: 3.9294 - val_MSE: 3.9294\n",
            "Epoch 999/1000\n",
            "908/908 [==============================] - 2s 2ms/step - loss: 3.9240 - MSE: 3.9240 - val_loss: 3.9410 - val_MSE: 3.9410\n",
            "Epoch 1000/1000\n",
            "908/908 [==============================] - 1s 2ms/step - loss: 3.9351 - MSE: 3.9351 - val_loss: 3.9415 - val_MSE: 3.9415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f420ac9c710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9B9B_AhsXei"
      },
      "source": [
        "The model have been trained our neural network on the entire dataset and and we have got Mean Square Error of 2.9988 and validation mean square error of 2.9988. The correlation between dependent variable(speed) and independent variable(lon, lat, height, Course and weight) in our dataset is very week.So That could be the main reason for the loss to be a little high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS6ywSjctjm1"
      },
      "source": [
        "Now, we can predict values with **predict** function. Test samples are passes to predict function of the keras trained model and you can compare the predicted value with the actual set (y_test).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PjrzxOnMo77"
      },
      "source": [
        "predictions = model.predict(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGBINEVs1k3k"
      },
      "source": [
        "**Now let see the Actual and predicted values in one table side by side.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLl7W0XNuieM"
      },
      "source": [
        "df_Actual_Predicted = pd.DataFrame({'Actual':y_test.to_numpy().tolist(),'Predicted':predictions.tolist()}, columns=['Actual','Predicted'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "TWlN-nRiu_HA",
        "outputId": "2abf25e3-c030-450d-f5e5-78010ee47d63"
      },
      "source": [
        "df_Actual_Predicted.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[20.0]</td>\n",
              "      <td>[19.040729522705078]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[18.67]</td>\n",
              "      <td>[19.47036361694336]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[19.39]</td>\n",
              "      <td>[19.266071319580078]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[30.0]</td>\n",
              "      <td>[24.886276245117188]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[18.48]</td>\n",
              "      <td>[19.34760284423828]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Actual             Predicted\n",
              "0   [20.0]  [19.040729522705078]\n",
              "1  [18.67]   [19.47036361694336]\n",
              "2  [19.39]  [19.266071319580078]\n",
              "3   [30.0]  [24.886276245117188]\n",
              "4  [18.48]   [19.34760284423828]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    }
  ]
}